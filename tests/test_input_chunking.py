#!/usr/bin/env python3
"""
æµ‹è¯•è¾“å…¥ç«¯åˆ†ç‰‡ç­–ç•¥

è¿è¡Œæ–¹å¼:
    python tests/test_input_chunking.py
"""
import asyncio
import os
import sys
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.reinvent_insight.model_config import GeminiClient, ModelConfig


def test_text_splitting():
    """æµ‹è¯•æ–‡æœ¬åˆ‡åˆ†åŠŸèƒ½"""
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„é…ç½®
    config = ModelConfig(
        task_type="text_to_speech",
        provider="gemini",
        model_name="gemini-2.5-flash-preview-tts",
        api_key="dummy_key",
        temperature=0.7,
        top_p=0.9,
        top_k=40,
        max_output_tokens=8000
    )
    client = GeminiClient(config)
    
    # æµ‹è¯•çŸ­æ–‡æœ¬ï¼ˆä¸éœ€è¦åˆ‡åˆ†ï¼‰
    short_text = "è¿™æ˜¯ä¸€ä¸ªçŸ­æ–‡æœ¬ã€‚"
    chunks = client._split_text_for_streaming(short_text, max_chunk_size=100)
    assert len(chunks) == 1, f"çŸ­æ–‡æœ¬åº”è¯¥åªæœ‰1ä¸ªç‰‡æ®µï¼Œä½†å¾—åˆ° {len(chunks)}"
    assert chunks[0] == short_text, "çŸ­æ–‡æœ¬å†…å®¹åº”è¯¥ä¿æŒä¸å˜"
    print(f"âœ… çŸ­æ–‡æœ¬æµ‹è¯•é€šè¿‡: {len(chunks)} ä¸ªç‰‡æ®µ")
    
    # æµ‹è¯•é•¿æ–‡æœ¬ï¼ˆéœ€è¦åˆ‡åˆ†ï¼‰
    long_text = """
    è¿™æ˜¯ç¬¬ä¸€ä¸ªå¥å­ã€‚è¿™æ˜¯ç¬¬äºŒä¸ªå¥å­ï¼è¿™æ˜¯ç¬¬ä¸‰ä¸ªå¥å­ï¼Ÿ
    è¿™æ˜¯ç¬¬å››ä¸ªå¥å­ã€‚è¿™æ˜¯ç¬¬äº”ä¸ªå¥å­ã€‚è¿™æ˜¯ç¬¬å…­ä¸ªå¥å­ã€‚
    è¿™æ˜¯ç¬¬ä¸ƒä¸ªå¥å­ã€‚è¿™æ˜¯ç¬¬å…«ä¸ªå¥å­ã€‚è¿™æ˜¯ç¬¬ä¹ä¸ªå¥å­ã€‚
    è¿™æ˜¯ç¬¬åä¸ªå¥å­ã€‚
    """
    chunks = client._split_text_for_streaming(long_text, max_chunk_size=50)
    assert len(chunks) > 1, f"é•¿æ–‡æœ¬åº”è¯¥è¢«åˆ‡åˆ†ä¸ºå¤šä¸ªç‰‡æ®µï¼Œä½†åªå¾—åˆ° {len(chunks)}"
    print(f"âœ… é•¿æ–‡æœ¬æµ‹è¯•é€šè¿‡: {len(chunks)} ä¸ªç‰‡æ®µ")
    
    # éªŒè¯æ¯ä¸ªç‰‡æ®µçš„é•¿åº¦
    for i, chunk in enumerate(chunks):
        print(f"   ç‰‡æ®µ {i+1}: {len(chunk)} å­—ç¬¦ - {chunk[:30]}...")
        # å¤§éƒ¨åˆ†ç‰‡æ®µåº”è¯¥ä¸è¶…è¿‡ max_chunk_sizeï¼ˆæœ€åä¸€ä¸ªå¯èƒ½è¾ƒçŸ­ï¼‰
        if i < len(chunks) - 1:
            assert len(chunk) <= 100, f"ç‰‡æ®µ {i+1} è¿‡é•¿: {len(chunk)} å­—ç¬¦"
    
    # æµ‹è¯•æ²¡æœ‰æ ‡ç‚¹ç¬¦å·çš„æ–‡æœ¬ï¼ˆå¼ºåˆ¶æŒ‰å­—ç¬¦åˆ‡åˆ†ï¼‰
    no_punctuation = "è¿™æ˜¯ä¸€æ®µæ²¡æœ‰ä»»ä½•æ ‡ç‚¹ç¬¦å·çš„å¾ˆé•¿çš„æ–‡æœ¬" * 10
    chunks = client._split_text_for_streaming(no_punctuation, max_chunk_size=50)
    assert len(chunks) > 1, "æ²¡æœ‰æ ‡ç‚¹çš„é•¿æ–‡æœ¬åº”è¯¥è¢«å¼ºåˆ¶åˆ‡åˆ†"
    print(f"âœ… æ— æ ‡ç‚¹æ–‡æœ¬æµ‹è¯•é€šè¿‡: {len(chunks)} ä¸ªç‰‡æ®µ")
    
    # éªŒè¯åˆ‡åˆ†åçš„æ–‡æœ¬æ‹¼æ¥å›å»åº”è¯¥ç­‰äºåŸæ–‡æœ¬
    rejoined = "".join(chunks)
    assert rejoined.strip() == no_punctuation.strip(), "åˆ‡åˆ†åæ‹¼æ¥åº”è¯¥ç­‰äºåŸæ–‡æœ¬"
    print("âœ… æ–‡æœ¬å®Œæ•´æ€§æµ‹è¯•é€šè¿‡")
    
    print("\nğŸ‰ æ‰€æœ‰æ–‡æœ¬åˆ‡åˆ†æµ‹è¯•é€šè¿‡ï¼")


async def test_streaming_with_chunking():
    """æµ‹è¯•å¸¦è¾“å…¥ç«¯åˆ†ç‰‡çš„æµå¼æ’­æ”¾"""
    # æ£€æŸ¥ API key
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        print("âš ï¸  è·³è¿‡å®é™… API æµ‹è¯•ï¼ˆéœ€è¦ GEMINI_API_KEYï¼‰")
        return
    
    # åˆ›å»ºå®¢æˆ·ç«¯
    config = ModelConfig(
        task_type="text_to_speech",
        provider="gemini",
        model_name="gemini-2.5-flash-preview-tts",
        api_key=api_key,
        temperature=0.7,
        top_p=0.9,
        top_k=40,
        max_output_tokens=8000
    )
    client = GeminiClient(config)
    
    # æµ‹è¯•æ–‡æœ¬ï¼ˆå¤šä¸ªå¥å­ï¼‰
    text = """
    è¿™æ˜¯ç¬¬ä¸€ä¸ªå¥å­ï¼Œç”¨äºæµ‹è¯•è¾“å…¥ç«¯åˆ†ç‰‡ç­–ç•¥ã€‚
    è¿™æ˜¯ç¬¬äºŒä¸ªå¥å­ï¼Œæˆ‘ä»¬æœŸæœ›èƒ½å¿«é€Ÿæ”¶åˆ°ç¬¬ä¸€ä¸ªç‰‡æ®µçš„éŸ³é¢‘ã€‚
    è¿™æ˜¯ç¬¬ä¸‰ä¸ªå¥å­ï¼Œç³»ç»Ÿåº”è¯¥æŒç»­å‘é€åç»­ç‰‡æ®µçš„éŸ³é¢‘ã€‚
    è¿™æ ·ç”¨æˆ·å°±èƒ½åœ¨å‡ ç§’é’Ÿå†…å¼€å§‹å¬åˆ°å£°éŸ³ï¼Œè€Œä¸æ˜¯ç­‰å¾…æ‰€æœ‰éŸ³é¢‘ç”Ÿæˆå®Œæˆã€‚
    """
    
    print("=" * 60)
    print("ğŸ¬ å¼€å§‹æµ‹è¯•è¾“å…¥ç«¯åˆ†ç‰‡æµå¼ TTS")
    print("=" * 60)
    print(f"ğŸ“ æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
    print()
    
    # è®°å½•æ—¶é—´
    start_time = time.time()
    first_chunk_time = None
    chunk_count = 0
    total_bytes = 0
    
    try:
        # è¿­ä»£æµ
        async for chunk in client.generate_tts_stream(text, "kore", "zh-CN"):
            current_time = time.time()
            
            if first_chunk_time is None:
                first_chunk_time = current_time
                first_chunk_latency = first_chunk_time - start_time
                print(f"âš¡ é¦–å—å»¶è¿Ÿ: {first_chunk_latency:.2f}s")
                print()
                
                if first_chunk_latency < 5.0:
                    print("ğŸ¯ å¤ªæ£’äº†ï¼é¦–å—å»¶è¿Ÿ < 5 ç§’ï¼Œè¾“å…¥ç«¯åˆ†ç‰‡ç­–ç•¥æˆåŠŸï¼")
                elif first_chunk_latency < 10.0:
                    print("âœ… ä¸é”™ï¼é¦–å—å»¶è¿Ÿ < 10 ç§’ï¼Œæ¯”ä¹‹å‰çš„ 15 ç§’æœ‰æ”¹å–„")
                else:
                    print(f"âš ï¸  é¦–å—å»¶è¿Ÿä»ç„¶è¾ƒé•¿: {first_chunk_latency:.2f}s")
                print()
            
            chunk_count += 1
            chunk_size = len(chunk)
            total_bytes += chunk_size
            
            elapsed = current_time - start_time
            print(f"ğŸ“¦ å— {chunk_count}: {chunk_size} bytes, ç´¯è®¡ {total_bytes / 1024:.1f}KB, è€—æ—¶ {elapsed:.2f}s")
        
        # å®Œæˆ
        total_time = time.time() - start_time
        print()
        print("=" * 60)
        print("ğŸ‰ è¾“å…¥ç«¯åˆ†ç‰‡æµå¼æ’­æ”¾æµ‹è¯•å®Œæˆ")
        print("=" * 60)
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   - æ€»å—æ•°: {chunk_count}")
        print(f"   - æ€»å¤§å°: {total_bytes / 1024:.1f}KB")
        print(f"   - æ€»æ—¶é•¿: {total_time:.2f}s")
        print(f"   - é¦–å—å»¶è¿Ÿ: {first_chunk_latency:.2f}s" if first_chunk_time else "   - é¦–å—å»¶è¿Ÿ: N/A")
        print("=" * 60)
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # è¿è¡Œæ–‡æœ¬åˆ‡åˆ†æµ‹è¯•ï¼ˆä¸éœ€è¦ API keyï¼‰
    test_text_splitting()
    
    print("\n" + "=" * 60 + "\n")
    
    # è¿è¡Œå®é™… API æµ‹è¯•ï¼ˆéœ€è¦ API keyï¼‰
    asyncio.run(test_streaming_with_chunking())
