"""
HTMLåˆ°Markdownè½¬æ¢å™¨ä¸»æ¥å£

æä¾›ç»Ÿä¸€çš„è½¬æ¢æ¥å£ï¼Œåè°ƒå„ç»„ä»¶å®ŒæˆHTMLåˆ°Markdownçš„è½¬æ¢ã€‚
"""

import logging
from pathlib import Path
from typing import Union, Optional

from reinvent_insight.infrastructure.ai.model_config import get_model_client
from .preprocessor import HTMLPreprocessor
from .extractor import LLMContentExtractor
from .url_processor import URLProcessor
from .generator import MarkdownGenerator
from .models import ConversionResult
from .exceptions import HTMLToMarkdownError

logger = logging.getLogger(__name__)


class HTMLToMarkdownConverter:
    """HTMLåˆ°Markdownè½¬æ¢å™¨ï¼ˆä¸»æ¥å£ï¼‰
    
    åè°ƒå„ç»„ä»¶å®ŒæˆHTMLåˆ°Markdownçš„è½¬æ¢æµç¨‹ã€‚
    
    ä½¿ç”¨ç¤ºä¾‹:
        >>> converter = HTMLToMarkdownConverter()
        >>> markdown = await converter.convert_from_file("article.html")
    """
    
    def __init__(self, task_type: str = "html_to_markdown", debug: bool = False):
        """åˆå§‹åŒ–è½¬æ¢å™¨
        
        Args:
            task_type: ä»»åŠ¡ç±»å‹ï¼ˆç”¨äºè·å–æ¨¡å‹é…ç½®ï¼‰
            debug: æ˜¯å¦å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼ˆä¿å­˜ä¸­é—´æ–‡ä»¶ï¼‰
        """
        self.task_type = task_type
        self.debug = debug
        
        # åˆå§‹åŒ–å„ç»„ä»¶
        logger.info(f"Initializing HTMLToMarkdownConverter with task_type={task_type}, debug={debug}")
        
        self.preprocessor = HTMLPreprocessor()
        self.model_client = get_model_client(task_type)
        self.extractor = LLMContentExtractor(self.model_client)
        self.url_processor = None  # å°†åœ¨è½¬æ¢æ—¶æ ¹æ®base_urlåˆå§‹åŒ–
        self.generator = MarkdownGenerator()
        
        logger.info("HTMLToMarkdownConverter initialized successfully")
    
    async def convert_from_file(
        self,
        html_path: Union[str, Path],
        output_path: Optional[Union[str, Path]] = None,
        base_url: Optional[str] = None
    ) -> ConversionResult:
        """ä»HTMLæ–‡ä»¶è½¬æ¢ä¸ºMarkdown
        
        Args:
            html_path: HTMLæ–‡ä»¶è·¯å¾„
            output_path: è¾“å‡ºMarkdownæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            base_url: ç½‘é¡µçš„åŸºç¡€URLï¼ˆç”¨äºå›¾ç‰‡è·¯å¾„è½¬æ¢ï¼‰
            
        Returns:
            ConversionResultå¯¹è±¡
            
        Raises:
            HTMLToMarkdownError: è½¬æ¢å¤±è´¥
        """
        logger.info(f"Converting from file: {html_path}")
        
        # è¯»å–HTMLæ–‡ä»¶
        html_path = Path(html_path)
        if not html_path.exists():
            raise HTMLToMarkdownError(f"HTML file not found: {html_path}")
        
        with open(html_path, 'r', encoding='utf-8') as f:
            html = f.read()
        
        # è½¬æ¢
        result = await self.convert_from_string(html, output_path, base_url)
        
        logger.info(f"Conversion from file completed: {html_path}")
        return result
    
    async def convert_from_url(
        self,
        url: str,
        output_path: Optional[Union[str, Path]] = None
    ) -> ConversionResult:
        """ä»URLè·å–HTMLå¹¶è½¬æ¢ä¸ºMarkdown
        
        Args:
            url: ç½‘é¡µURL
            output_path: è¾“å‡ºMarkdownæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            ConversionResultå¯¹è±¡
            
        Raises:
            HTMLToMarkdownError: è½¬æ¢å¤±è´¥
        """
        logger.info(f"Converting from URL: {url}")
        
        try:
            import httpx
        except ImportError:
            raise HTMLToMarkdownError(
                "httpx is required for URL fetching. Install it with: pip install httpx"
            )
        
        # è·å–HTML
        try:
            print("ğŸ“¡ æ­£åœ¨è·å–ç½‘é¡µ...")
            # å¢åŠ è¶…æ—¶æ—¶é—´ï¼ŒSemiAnalysisç½‘é¡µè¾ƒå¤§
            timeout_config = httpx.Timeout(60.0, connect=10.0)
            async with httpx.AsyncClient(timeout=timeout_config, follow_redirects=True) as client:
                print("   å‘é€HTTPè¯·æ±‚...")
                response = await client.get(url)
                response.raise_for_status()
                html = response.text
                html_size_mb = len(html) / (1024 * 1024)
                print(f"âœ… è·å–æˆåŠŸï¼HTMLå¤§å°: {html_size_mb:.2f} MB ({len(html):,} å­—ç¬¦)")
        except Exception as e:
            logger.error(f"Failed to fetch URL {url}: {e}")
            print(f"âŒ è·å–å¤±è´¥: {e}")
            raise HTMLToMarkdownError(f"Failed to fetch URL: {e}") from e
        
        # ä½¿ç”¨URLä½œä¸ºbase_url
        result = await self.convert_from_string(html, output_path, base_url=url)
        
        logger.info(f"Conversion from URL completed: {url}")
        return result
    
    async def convert_from_string(
        self,
        html: str,
        output_path: Optional[Union[str, Path]] = None,
        base_url: Optional[str] = None
    ) -> ConversionResult:
        """ä»HTMLå­—ç¬¦ä¸²è½¬æ¢ä¸ºMarkdown
        
        Args:
            html: HTMLå­—ç¬¦ä¸²
            output_path: è¾“å‡ºMarkdownæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            base_url: ç½‘é¡µçš„åŸºç¡€URLï¼ˆç”¨äºå›¾ç‰‡è·¯å¾„è½¬æ¢ï¼‰
            
        Returns:
            ConversionResultå¯¹è±¡
            
        Raises:
            HTMLToMarkdownError: è½¬æ¢å¤±è´¥
        """
        logger.info("Converting from HTML string")
        logger.info(f"HTML length: {len(html)} chars, base_url: {base_url}")
        
        try:
            # å¦‚æœå¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œä¿å­˜åŸå§‹HTML
            if self.debug and output_path:
                debug_dir = Path(output_path).parent / "debug"
                debug_dir.mkdir(exist_ok=True)
                
                original_html_path = debug_dir / f"{Path(output_path).stem}_01_original.html"
                with open(original_html_path, 'w', encoding='utf-8') as f:
                    f.write(html)
                logger.info(f"Debug: Saved original HTML to {original_html_path}")
            
            # æ­¥éª¤1: é¢„å¤„ç†HTML
            print("\nğŸ§½ æ­¥éª¤1: é¢„å¤„ç†HTMLï¼ˆå»é™¤JS/CSS/å¹¿å‘Šç­‰ï¼‰...")
            logger.info("Step 1: Preprocessing HTML...")
            cleaned_html = self.preprocessor.preprocess(html)
            cleaned_size_mb = len(cleaned_html) / (1024 * 1024)
            reduction = (1 - len(cleaned_html) / len(html)) * 100
            print(f"   âœ… é¢„å¤„ç†å®Œæˆ: {cleaned_size_mb:.2f} MB ({len(cleaned_html):,} å­—ç¬¦)")
            print(f"   ğŸ“‰ å‹ç¼©ç‡: {reduction:.1f}%")
            logger.info(f"HTML preprocessed: {len(cleaned_html)} chars")
            
            # å¦‚æœå¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œä¿å­˜é¢„å¤„ç†åçš„HTML
            if self.debug and output_path:
                cleaned_html_path = debug_dir / f"{Path(output_path).stem}_02_cleaned.html"
                with open(cleaned_html_path, 'w', encoding='utf-8') as f:
                    f.write(cleaned_html)
                logger.info(f"Debug: Saved cleaned HTML to {cleaned_html_path}")
            
            # æ­¥éª¤2: LLMæå–å†…å®¹
            print("\nğŸ¤– æ­¥éª¤2: ä½¿ç”¨ Gemini æå–æ­£æ–‡å†…å®¹...")
            logger.info("Step 2: Extracting content with LLM...")
            
            # å¦‚æœå¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œä¼ é€’è°ƒè¯•ç›®å½•ç»™extractor
            if self.debug and output_path:
                self.extractor.debug_dir = debug_dir
                self.extractor.output_stem = Path(output_path).stem
            
            extracted_content = await self.extractor.extract(cleaned_html, base_url)
            logger.info(f"Content extracted: title='{extracted_content.title}', "
                       f"content_length={len(extracted_content.content)}, "
                       f"images={len(extracted_content.images)}")
            
            # å¦‚æœå¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œä¿å­˜æå–çš„å†…å®¹ï¼ˆJSONæ ¼å¼ï¼‰
            if self.debug and output_path:
                import json
                extracted_json_path = debug_dir / f"{Path(output_path).stem}_03_extracted.json"
                with open(extracted_json_path, 'w', encoding='utf-8') as f:
                    json.dump(extracted_content.to_dict(), f, ensure_ascii=False, indent=2)
                logger.info(f"Debug: Saved extracted content to {extracted_json_path}")
            
            # æ­¥éª¤3: å¤„ç†å›¾ç‰‡URL
            if base_url and extracted_content.images:
                logger.info("Step 3: Processing image URLs...")
                url_processor = URLProcessor(base_url)
                
                for image in extracted_content.images:
                    try:
                        original_url = image.url
                        processed_url = url_processor.process_image_url(original_url)
                        image.url = processed_url
                        
                        if original_url != processed_url:
                            logger.debug(f"URL processed: {original_url} -> {processed_url}")
                    except Exception as e:
                        logger.warning(f"Failed to process image URL {image.url}: {e}")
                        # ç»§ç»­å¤„ç†å…¶ä»–å›¾ç‰‡
                
                logger.info(f"Processed {len(extracted_content.images)} image URLs")
            else:
                logger.info("Step 3: Skipping URL processing (no base_url or no images)")
            
            # æ­¥éª¤4: ç”ŸæˆMarkdown
            logger.info("Step 4: Generating Markdown...")
            output_path_obj = Path(output_path) if output_path else None
            result = self.generator.generate(extracted_content, output_path_obj)
            logger.info(f"Markdown generated: {len(result.markdown)} chars")
            
            logger.info("Conversion completed successfully")
            return result
            
        except Exception as e:
            logger.error(f"Conversion failed: {e}", exc_info=True)
            raise
