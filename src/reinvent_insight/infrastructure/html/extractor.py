"""
LLMå†…å®¹æå–å™¨

ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½æå–HTMLä¸­çš„æ ¸å¿ƒå†…å®¹ã€‚
"""

import json
import logging
from pathlib import Path
from typing import Optional, List

from bs4 import BeautifulSoup

from reinvent_insight.infrastructure.ai.model_config import BaseModelClient
from .models import ExtractedContent, ImageInfo
from .exceptions import LLMProcessingError, ContentExtractionError

logger = logging.getLogger(__name__)


class LLMContentExtractor:
    """LLMå†…å®¹æå–å™¨
    
    ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½æå–HTMLä¸­çš„ï¼š
    - æ–‡ç« æ ‡é¢˜
    - æ­£æ–‡å†…å®¹
    - ç›¸å…³å›¾ç‰‡
    - å…ƒæ•°æ®ï¼ˆä½œè€…ã€æ—¥æœŸç­‰ï¼‰
    
    åŒæ—¶è¿‡æ»¤æ‰å¹¿å‘Šã€å¯¼èˆªç­‰æ— å…³å†…å®¹ã€‚
    
    ä½¿ç”¨ç¤ºä¾‹:
        >>> from reinvent_insight.infrastructure.ai.model_config import get_model_client
        >>> client = get_model_client("html_to_markdown")
        >>> extractor = LLMContentExtractor(client)
        >>> content = await extractor.extract(html, base_url="https://example.com")
    """
    
    def __init__(self, model_client: BaseModelClient):
        """åˆå§‹åŒ–æå–å™¨
        
        Args:
            model_client: æ¨¡å‹å®¢æˆ·ç«¯ï¼ˆæ¥è‡ªç»Ÿä¸€é…ç½®ç³»ç»Ÿï¼‰
        """
        self.model_client = model_client
        self.prompt_template = self._load_prompt_template()
        self.debug_dir = None  # è°ƒè¯•ç›®å½•
        self.output_stem = None  # è¾“å‡ºæ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
        logger.info("LLMContentExtractor initialized")
    
    def _load_prompt_template(self) -> str:
        """åŠ è½½æç¤ºè¯æ¨¡æ¿
        
        Returns:
            æç¤ºè¯æ¨¡æ¿å­—ç¬¦ä¸²
        """
        # è·å–é¡¹ç›®æ ¹ç›®å½•
        from reinvent_insight.core import config as app_config
        prompt_path = app_config.PROJECT_ROOT / "prompt" / "html_to_markdown.txt"
        
        if not prompt_path.exists():
            logger.error(f"Prompt template not found: {prompt_path}")
            raise LLMProcessingError(f"Prompt template not found: {prompt_path}")
        
        with open(prompt_path, 'r', encoding='utf-8') as f:
            template = f.read()
        
        logger.info(f"Loaded prompt template from {prompt_path}")
        return template
    
    async def extract(
        self, 
        html: str,
        base_url: Optional[str] = None
    ) -> ExtractedContent:
        """ä»HTMLä¸­æå–å†…å®¹
        
        Args:
            html: é¢„å¤„ç†åçš„HTML
            base_url: ç½‘é¡µçš„åŸºç¡€URLï¼ˆç”¨äºå›¾ç‰‡è·¯å¾„è½¬æ¢ï¼‰
            
        Returns:
            æå–çš„å†…å®¹å¯¹è±¡
            
        Raises:
            LLMProcessingError: LLMå¤„ç†å¤±è´¥
            ContentExtractionError: å†…å®¹æå–å¤±è´¥
        """
        logger.info("Starting content extraction with LLM")
        
        # æ£€æŸ¥HTMLé•¿åº¦,å†³å®šæ˜¯å¦éœ€è¦åˆ†æ®µå¤„ç†
        # ç›®æ ‡ï¼šæ¯æ®µçº¦3000-5000æ±‰å­—çš„å†…å®¹ï¼ˆéå¸¸ä¿å®ˆçš„è®¾ç½®ï¼‰
        # 30000å­—ç¬¦çš„HTML â‰ˆ 3000-5000æ±‰å­—çš„å†…å®¹ï¼ˆè€ƒè™‘HTMLæ ‡ç­¾å¼€é”€ï¼‰
        # å¯¹äºç‰¹åˆ«å¤§çš„HTMLï¼ˆå¦‚SemiAnalysisï¼‰ï¼Œä½¿ç”¨æ›´å°çš„åˆ†æ®µç¡®ä¿APIä¸è¶…æ—¶
        max_chunk_size = 30000  # æ¯æ®µæœ€å¤§30KBï¼ˆéå¸¸ä¿å®ˆï¼Œé¿å…ä»»ä½•è¶…æ—¶é£é™©ï¼‰
        
        if len(html) <= max_chunk_size:
            # å•æ¬¡å¤„ç†
            logger.info(f"HTML size ({len(html)} chars) within limit, processing in single pass")
            return await self._extract_single(html, base_url)
        else:
            # åˆ†æ®µå¤„ç†
            logger.info(f"HTML size ({len(html)} chars) exceeds limit, using chunked processing")
            return await self._extract_chunked(html, base_url, max_chunk_size)
    
    async def _extract_single(
        self,
        html: str,
        base_url: Optional[str] = None
    ) -> ExtractedContent:
        """å•æ¬¡æå–ï¼ˆä¸åˆ†æ®µï¼‰
        
        Args:
            html: HTMLå†…å®¹
            base_url: åŸºç¡€URL
            
        Returns:
            æå–çš„å†…å®¹å¯¹è±¡
        """
        # æ„å»ºæç¤ºè¯
        prompt = self._build_prompt(html)
        
        try:
            # è°ƒç”¨LLM APIï¼ˆä½¿ç”¨JSONæ¨¡å¼ï¼Œthinking_levelç”±é…ç½®å†³å®šï¼‰
            logger.info("Calling LLM API...")
            response = await self.model_client.generate_content(
                prompt=prompt,
                is_json=True
            )
            
            logger.info("LLM API call successful")
            
            # è§£æå“åº”
            content = self._parse_llm_response(response)
            
            # éªŒè¯æå–çš„å†…å®¹
            self._validate_content(content)
            
            logger.info(f"Content extraction completed: title='{content.title}', "
                       f"images={len(content.images)}")
            
            return content
            
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse LLM response as JSON: {e}")
            logger.debug(f"Response content: {response[:500]}...")
            raise LLMProcessingError(f"LLM returned invalid JSON: {e}") from e
        
        except Exception as e:
            logger.error(f"Content extraction failed: {e}", exc_info=True)
            raise LLMProcessingError(f"Content extraction failed: {e}") from e
    
    async def _extract_chunked(
        self,
        html: str,
        base_url: Optional[str] = None,
        max_chunk_size: int = 30000  # é»˜è®¤30KBï¼Œæåº¦ä¿å®ˆé¿å…è¶…æ—¶
    ) -> ExtractedContent:
        """åˆ†æ®µæå–å¹¶åˆå¹¶ï¼ˆä½¿ç”¨å¹¶å‘å¤„ç†æå‡æ•ˆç‡ï¼‰
        
        Args:
            html: HTMLå†…å®¹
            base_url: åŸºç¡€URL
            max_chunk_size: æ¯æ®µæœ€å¤§å¤§å°
            
        Returns:
            åˆå¹¶åçš„å†…å®¹å¯¹è±¡
        """
        import asyncio
        from bs4 import BeautifulSoup
        
        # è§£æHTML
        soup = BeautifulSoup(html, 'lxml')
        
        # æå–æ‰€æœ‰æ®µè½çº§å…ƒç´ 
        chunks = self._split_html_semantically(soup, max_chunk_size)
        
        logger.info(f"Split HTML into {len(chunks)} chunks")
        print(f"\nğŸ“¦ å°†HTMLåˆ†ä¸º {len(chunks)} ä¸ªåˆ†æ®µï¼Œå‡†å¤‡å¹¶å‘å¤„ç†...")
        
        # å¹¶å‘å¤„ç†æ‰€æœ‰åˆ†æ®µ
        tasks = []
        # ä»é…ç½®ä¸­è·å–å¹¶å‘é—´éš”æ—¶é—´ï¼Œé»˜è®¤0.5ç§’
        concurrent_delay = getattr(self.model_client.config, 'concurrent_delay', 0.5)
        logger.info(f"Using concurrent_delay={concurrent_delay} seconds from config")
        
        for i, chunk_html in enumerate(chunks):
            # æ¯ä¸ªä»»åŠ¡å»¶è¿Ÿå¯åŠ¨ï¼Œé¿å…åŒæ—¶å‘é€æ‰€æœ‰è¯·æ±‚
            delay = i * concurrent_delay  # ä»é…ç½®è¯»å–é—´éš”æ—¶é—´
            task = self._process_chunk_with_delay(
                chunk_html, i, len(chunks), delay
            )
            tasks.append(task)
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        print(f"\nâš¡ å¼€å§‹å¹¶å‘å¤„ç† {len(chunks)} ä¸ªåˆ†æ®µ...")
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # æ”¶é›†ç»“æœ
        all_contents = []
        all_images = []
        title = ""
        metadata = {}
        
        successful_chunks = 0
        failed_chunks = 0
        
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"Chunk {i+1} failed: {result}")
                failed_chunks += 1
                continue
            
            if result is None:
                logger.warning(f"Chunk {i+1} returned None")
                failed_chunks += 1
                continue
            
            chunk_content = result
            successful_chunks += 1
            
            # ç¬¬ä¸€æ®µæå–æ ‡é¢˜å’Œå…ƒæ•°æ®
            if i == 0:
                title = chunk_content.title
                metadata = chunk_content.metadata
            
            # æ”¶é›†å†…å®¹å’Œå›¾ç‰‡
            all_contents.append(chunk_content.content)
            all_images.extend(chunk_content.images)
            
            logger.info(f"Chunk {i+1} processed: {len(chunk_content.content)} chars, "
                       f"{len(chunk_content.images)} images")
        
        print(f"\nâœ… å¹¶å‘å¤„ç†å®Œæˆ: {successful_chunks}/{len(chunks)} æˆåŠŸ, {failed_chunks} å¤±è´¥")
        
        if not all_contents:
            raise ContentExtractionError("æ‰€æœ‰åˆ†æ®µå¤„ç†éƒ½å¤±è´¥äº†ï¼Œæ— æ³•æå–å†…å®¹")
        
        # åˆå¹¶æ‰€æœ‰å†…å®¹
        merged_content = self._merge_contents(all_contents)
        
        # å¦‚æœå¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œä¿å­˜åˆå¹¶åçš„å†…å®¹
        if self.debug_dir and self.output_stem:
            merged_md_path = self.debug_dir / f"{self.output_stem}_merged_content.md"
            with open(merged_md_path, 'w', encoding='utf-8') as f:
                f.write(f"# Merged Content from {len(chunks)} chunks\n\n")
                f.write(merged_content)
            logger.info(f"Debug: Saved merged content to {merged_md_path}")
        
        # å»é‡å›¾ç‰‡
        unique_images = self._deduplicate_images(all_images)
        
        # åˆ›å»ºæœ€ç»ˆçš„ExtractedContentå¯¹è±¡
        final_content = ExtractedContent(
            title=title,
            content=merged_content,
            images=unique_images,
            metadata=metadata
        )
        
        logger.info(f"Chunked extraction completed: {len(merged_content)} chars total, "
                   f"{len(unique_images)} unique images")
        
        return final_content
    
    async def _process_chunk_with_delay(
        self,
        chunk_html: str,
        chunk_index: int,
        total_chunks: int,
        delay: float
    ) -> Optional[ExtractedContent]:
        """å»¶è¿Ÿå¤„ç†å•ä¸ªåˆ†æ®µ
        
        Args:
            chunk_html: åˆ†æ®µHTML
            chunk_index: åˆ†æ®µç´¢å¼•
            total_chunks: æ€»åˆ†æ®µæ•°
            delay: å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
            
        Returns:
            æå–çš„å†…å®¹ï¼Œå¤±è´¥è¿”å›None
        """
        import asyncio
        
        # å»¶è¿Ÿå¯åŠ¨
        if delay > 0:
            await asyncio.sleep(delay)
        
        print(f"\nğŸ“ [åˆ†æ®µ {chunk_index+1}/{total_chunks}] å¼€å§‹å¤„ç† ({len(chunk_html):,} å­—ç¬¦)...")
        logger.info(f"Processing chunk {chunk_index+1}/{total_chunks} ({len(chunk_html)} chars)")
        
        # å¦‚æœå¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œä¿å­˜åˆ†æ®µHTML
        if self.debug_dir and self.output_stem:
            chunk_html_path = self.debug_dir / f"{self.output_stem}_chunk_{chunk_index+1:02d}_html.html"
            with open(chunk_html_path, 'w', encoding='utf-8') as f:
                f.write(chunk_html)
            logger.info(f"Debug: Saved chunk {chunk_index+1} HTML to {chunk_html_path}")
        
        try:
            # ä¸ºåˆ†æ®µåˆ›å»ºç‰¹æ®Šæç¤ºè¯
            chunk_prompt = self._build_chunk_prompt(chunk_html, chunk_index, total_chunks)
            
            print(f"   â³ [åˆ†æ®µ {chunk_index+1}] è°ƒç”¨ Gemini API...")
            # thinking_levelç”±é…ç½®å†³å®š
            response = await self.model_client.generate_content(
                prompt=chunk_prompt,
                is_json=True
            )
            print(f"   âœ… [åˆ†æ®µ {chunk_index+1}] å¤„ç†å®Œæˆ")
            
            chunk_content = self._parse_llm_response(response)
            
            # å¦‚æŸå¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œä¿å­˜åˆ†æ®µæå–ç»“æœ
            if self.debug_dir and self.output_stem:
                chunk_json_path = self.debug_dir / f"{self.output_stem}_chunk_{chunk_index+1:02d}_extracted.json"
                with open(chunk_json_path, 'w', encoding='utf-8') as f:
                    json.dump(chunk_content.to_dict(), f, ensure_ascii=False, indent=2)
                logger.info(f"Debug: Saved chunk {chunk_index+1} extraction to {chunk_json_path}")
                
                # ä¿å­˜åˆ†æ®µçš„Markdown
                chunk_md_path = self.debug_dir / f"{self.output_stem}_chunk_{chunk_index+1:02d}_content.md"
                with open(chunk_md_path, 'w', encoding='utf-8') as f:
                    f.write(f"# Chunk {chunk_index+1}/{total_chunks}\n\n")
                    if chunk_index == 0 and chunk_content.title:
                        f.write(f"## Title: {chunk_content.title}\n\n")
                    f.write(chunk_content.content)
                logger.info(f"Debug: Saved chunk {chunk_index+1} markdown to {chunk_md_path}")
            
            print(f"   ğŸ“Š [åˆ†æ®µ {chunk_index+1}] æå–äº† {len(chunk_content.images)} å¼ å›¾ç‰‡")
            return chunk_content
            
        except Exception as e:
            logger.error(f"Failed to process chunk {chunk_index+1}: {e}")
            print(f"   âŒ [åˆ†æ®µ {chunk_index+1}] å¤„ç†å¤±è´¥: {e}")
            return None
    
    def _split_html_semantically(
        self,
        soup: 'BeautifulSoup',
        max_size: int
    ) -> List[str]:
        """æŒ‰è¯­ä¹‰åˆ†å‰²HTML
        
        Args:
            soup: BeautifulSoupå¯¹è±¡
            max_size: æ¯æ®µæœ€å¤§å¤§å°
            
        Returns:
            HTMLåˆ†æ®µåˆ—è¡¨
        """
        chunks = []
        current_chunk = []
        current_size = 0
        overlap_size = 1000  # é‡å 1KBç”¨äºä¸Šä¸‹æ–‡ï¼ˆçº¦200-300æ±‰å­—ï¼‰
        
        # è·å–bodyä¸­çš„æ‰€æœ‰é¡¶çº§å…ƒç´ 
        body = soup.find('body')
        if not body:
            return [str(soup)]
        
        # è·å–æ‰€æœ‰é¡¶çº§å®¹å™¨å…ƒç´ ï¼ˆåªè·å–bodyçš„ç›´æ¥å­å…ƒç´ ï¼Œä¿æŒç»“æ„å®Œæ•´ï¼‰
        # è¿™æ ·å¯ä»¥ä¿ç•™åµŒå¥—åœ¨å®¹å™¨å†…çš„å›¾ç‰‡
        elements = body.find_all(['div', 'article', 'section', 'main'], recursive=False)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å®¹å™¨å…ƒç´ ï¼Œå°è¯•æŸ¥æ‰¾æ®µè½çº§å…ƒç´ 
        if not elements:
            elements = body.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 
                                      'ul', 'ol', 'blockquote', 'pre', 'table'], recursive=False)
        
        # å¦‚æœè¿˜æ˜¯æ²¡æœ‰æ‰¾åˆ°ï¼Œé€’å½’æŸ¥æ‰¾æ‰€æœ‰æ®µè½çº§å…ƒç´ ï¼ˆè¿™ä¼šä¿ç•™å›¾ç‰‡ï¼‰
        if not elements:
            elements = body.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
        
        if not elements:
            # å¦‚æœè¿˜æ˜¯æ²¡æœ‰ï¼Œè¿”å›æ•´ä¸ªbodyï¼ˆè¿™ä¼šä¿ç•™æ‰€æœ‰å›¾ç‰‡ï¼‰
            return [str(body)]
        
        logger.info(f"Found {len(elements)} elements to split")
        
        for i, elem in enumerate(elements):
            elem_str = str(elem)
            elem_size = len(elem_str)
            
            # å¦‚æœå•ä¸ªå…ƒç´ å°±è¶…è¿‡é™åˆ¶ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†å‰²
            if elem_size > max_size:
                logger.warning(f"Element {i} is too large ({elem_size} chars), will be split")
                
                # ä¿å­˜å½“å‰chunk
                if current_chunk:
                    chunks.append(''.join(current_chunk))
                    current_chunk = []
                    current_size = 0
                
                # å¯¹å¤§å…ƒç´ è¿›è¡Œæ–‡æœ¬çº§åˆ†å‰²
                sub_chunks = self._split_large_element(elem_str, max_size)
                chunks.extend(sub_chunks)
                continue
            
            # å¦‚æœåŠ ä¸Šå½“å‰å…ƒç´ ä¼šè¶…è¿‡é™åˆ¶
            if current_size + elem_size > max_size and current_chunk:
                # ä¿å­˜å½“å‰åˆ†æ®µ
                chunk_content = ''.join(current_chunk)
                chunks.append(chunk_content)
                logger.debug(f"Created chunk {len(chunks)} with {len(chunk_content)} chars")
                
                # å¼€å§‹æ–°åˆ†æ®µï¼ŒåŒ…å«é‡å å†…å®¹ï¼ˆæœ€åå‡ ä¸ªå…ƒç´ ï¼‰
                overlap_elements = []
                overlap_len = 0
                for prev_elem in reversed(current_chunk[-5:]):  # æœ€å¤šä¿ç•™æœ€å5ä¸ªå…ƒç´ 
                    if overlap_len + len(prev_elem) < overlap_size:
                        overlap_elements.insert(0, prev_elem)
                        overlap_len += len(prev_elem)
                    else:
                        break
                
                current_chunk = overlap_elements
                current_size = overlap_len
            
            current_chunk.append(elem_str)
            current_size += elem_size
        
        # æ·»åŠ æœ€åä¸€æ®µ
        if current_chunk:
            chunk_content = ''.join(current_chunk)
            chunks.append(chunk_content)
            logger.debug(f"Created final chunk {len(chunks)} with {len(chunk_content)} chars")
        
        return chunks
    
    def _split_large_element(self, elem_str: str, max_size: int) -> List[str]:
        """åˆ†å‰²è¿‡å¤§çš„å•ä¸ªå…ƒç´ 
        
        Args:
            elem_str: å…ƒç´ å­—ç¬¦ä¸²
            max_size: æœ€å¤§å¤§å°
            
        Returns:
            åˆ†å‰²åçš„å­—ç¬¦ä¸²åˆ—è¡¨
        """
        # ç®€å•æŒ‰å­—ç¬¦æ•°åˆ†å‰²ï¼Œä¿ç•™ä¸€äº›é‡å 
        chunks = []
        overlap = 500  # å‡å°é‡å å¤§å°
        start = 0
        
        while start < len(elem_str):
            end = start + max_size
            chunk = elem_str[start:end]
            chunks.append(chunk)
            start = end - overlap  # é‡å éƒ¨åˆ†
        
        return chunks
    
    def _build_chunk_prompt(self, html: str, chunk_index: int, total_chunks: int) -> str:
        """ä¸ºåˆ†æ®µæ„å»ºæç¤ºè¯
        
        Args:
            html: HTMLåˆ†æ®µ
            chunk_index: å½“å‰åˆ†æ®µç´¢å¼•
            total_chunks: æ€»åˆ†æ®µæ•°
            
        Returns:
            æç¤ºè¯
        """
        if chunk_index == 0:
            # ç¬¬ä¸€æ®µï¼šæå–æ ‡é¢˜ã€å…ƒæ•°æ®å’Œå†…å®¹
            prefix = f"""è¿™æ˜¯æ–‡ç« çš„ç¬¬ {chunk_index + 1}/{total_chunks} éƒ¨åˆ†ï¼ˆå¼€å¤´éƒ¨åˆ†ï¼‰ã€‚
è¯·æå–æ ‡é¢˜ã€å…ƒæ•°æ®å’Œè¿™éƒ¨åˆ†çš„å®Œæ•´å†…å®¹ã€‚

**é‡è¦ï¼š**ä¸‹ä¸€ä¸ªåˆ†æ®µä¼šä»¥è¿™ä¸ªåˆ†æ®µçš„ç»“å°¾å†…å®¹å¼€å§‹ï¼ˆæœ‰é‡å ï¼‰ï¼Œè¯·ç¡®ä¿ç¿»è¯‘ä¸€è‡´ã€‚

"""
        elif chunk_index == total_chunks - 1:
            # æœ€åä¸€æ®µï¼šåªæå–å†…å®¹
            prefix = f"""è¿™æ˜¯æ–‡ç« çš„ç¬¬ {chunk_index + 1}/{total_chunks} éƒ¨åˆ†ï¼ˆç»“å°¾éƒ¨åˆ†ï¼‰ã€‚
è¯·æå–è¿™éƒ¨åˆ†çš„å®Œæ•´å†…å®¹ã€‚æ ‡é¢˜å’Œå…ƒæ•°æ®å¯ä»¥ç•™ç©ºã€‚

**é‡è¦ï¼š**è¿™ä¸ªåˆ†æ®µçš„å¼€å¤´ä¸ä¸Šä¸€ä¸ªåˆ†æ®µçš„ç»“å°¾æœ‰é‡å ã€‚å¦‚æœä½ è¯†åˆ«å‡ºå¼€å¤´çš„å†…å®¹å·²ç»åœ¨ä¸Šä¸€æ®µä¸­å‡ºç°ï¼Œè¯·ç›´æ¥è·³è¿‡ï¼Œä»æ–°å†…å®¹å¼€å§‹ç¿»è¯‘ã€‚ä¸è¦æ·»åŠ â€œï¼ˆæ¥ä¸Šæ–‡ï¼‰â€ç­‰æ ‡è®°ã€‚

"""
        else:
            # ä¸­é—´æ®µï¼šåªæå–å†…å®¹
            prefix = f"""è¿™æ˜¯æ–‡ç« çš„ç¬¬ {chunk_index + 1}/{total_chunks} éƒ¨åˆ†ï¼ˆä¸­é—´éƒ¨åˆ†ï¼‰ã€‚
è¯·æå–è¿™éƒ¨åˆ†çš„å®Œæ•´å†…å®¹ã€‚æ ‡é¢˜å’Œå…ƒæ•°æ®å¯ä»¥ç•™ç©ºã€‚

**é‡è¦ï¼š**è¿™ä¸ªåˆ†æ®µçš„å¼€å¤´ä¸ä¸Šä¸€ä¸ªåˆ†æ®µçš„ç»“å°¾æœ‰é‡å ï¼Œç»“å°¾ä¸ä¸‹ä¸€ä¸ªåˆ†æ®µçš„å¼€å¤´ä¹Ÿæœ‰é‡å ã€‚å¦‚æœä½ è¯†åˆ«å‡ºå¼€å¤´çš„å†…å®¹å·²ç»åœ¨ä¸Šä¸€æ®µä¸­å‡ºç°ï¼Œè¯·ç›´æ¥è·³è¿‡ï¼Œä»æ–°å†…å®¹å¼€å§‹ç¿»è¯‘ã€‚ä¸è¦æ·»åŠ â€œï¼ˆæ¥ä¸Šæ–‡ï¼‰â€ç­‰æ ‡è®°ã€‚è¯·ç¡®ä¿ç¿»è¯‘ä¸å‰ååˆ†æ®µä¿æŒä¸€è‡´ã€‚

"""
        
        return prefix + self.prompt_template.replace("{html}", html)
    
    def _merge_contents(self, contents: List[str]) -> str:
        """åˆå¹¶å¤šä¸ªå†…å®¹æ®µï¼Œå»é™¤é‡å¤
        
        Args:
            contents: å†…å®¹åˆ—è¡¨
            
        Returns:
            åˆå¹¶åçš„å†…å®¹
        """
        if not contents:
            return ""
        
        if len(contents) == 1:
            return contents[0]
        
        merged = contents[0]
        
        for i in range(1, len(contents)):
            next_content = contents[i]
            
            # æŸ¥æ‰¾é‡å éƒ¨åˆ†ï¼ˆæœ€åå‡ æ®µå’Œå¼€å¤´å‡ æ®µï¼‰
            merged_lines = merged.split('\n')
            next_lines = next_content.split('\n')
            
            # å°è¯•æ‰¾åˆ°é‡å çš„è¡Œ
            overlap_found = False
            # ä»è¾ƒå¤§çš„é‡å å¼€å§‹å°è¯•ï¼Œæœ€å¤šæ£€æŸ¥30è¡Œ
            max_overlap = min(30, len(merged_lines), len(next_lines))
            for overlap_len in range(max_overlap, 2, -1):  # æœ€å°‘é‡å 3è¡Œ
                merged_tail = '\n'.join(merged_lines[-overlap_len:])
                next_head = '\n'.join(next_lines[:overlap_len])
                
                # ä½¿ç”¨æ›´ä¸¥æ ¼çš„åŒ¹é…ï¼šåŸºäºå®Œæ•´å¥å­åŒ¹é…
                if self._is_overlapping_content(merged_tail, next_head):
                    # æ‰¾åˆ°é‡å ï¼Œè·³è¿‡é‡å¤éƒ¨åˆ†
                    remaining_lines = next_lines[overlap_len:]
                    if remaining_lines:  # ç¡®ä¿æœ‰å‰©ä½™å†…å®¹
                        merged += '\n' + '\n'.join(remaining_lines)
                    overlap_found = True
                    logger.debug(f"Found overlap of {overlap_len} lines between chunk {i} and {i+1}")
                    break
            
            if not overlap_found:
                # æ²¡æ‰¾åˆ°é‡å ï¼Œç›´æ¥æ‹¼æ¥
                merged += '\n\n' + next_content
                logger.debug(f"No overlap found between chunk {i} and {i+1}, direct concatenation")
        
        return merged
    
    def _is_overlapping_content(self, text1: str, text2: str) -> bool:
        """åˆ¤æ–­ä¸¤æ®µæ–‡æœ¬æ˜¯å¦ä¸ºé‡å å†…å®¹
        
        Args:
            text1: æ–‡æœ¬1ï¼ˆå‰ä¸€æ®µçš„ç»“å°¾ï¼‰
            text2: æ–‡æœ¬2ï¼ˆåä¸€æ®µçš„å¼€å¤´ï¼‰
            
        Returns:
            æ˜¯å¦ä¸ºé‡å å†…å®¹
        """
        if not text1 or not text2:
            return False
        
        # ç§»é™¤â€œï¼ˆæ¥ä¸Šæ–‡ï¼‰â€ç­‰æç¤ºæ€§æ–‡å­—
        text2_clean = text2.replace('ï¼ˆæ¥ä¸Šæ–‡ï¼‰', '').replace('(æ¥ä¸Šæ–‡)', '')
        text2_clean = text2_clean.replace('...', '').strip()
        
        # ç§»é™¤ç©ºç™½å­—ç¬¦åæ¯”è¾ƒ
        clean1 = ''.join(text1.split())
        clean2 = ''.join(text2_clean.split())
        
        if not clean1 or not clean2:
            return False
        
        # å¦‚æœå†…å®¹å®Œå…¨ç›¸åŒï¼Œè®¤ä¸ºæ˜¯é‡å 
        if clean1 == clean2:
            return True
        
        # æå–å…³é”®è¯å¥ï¼ˆç§»é™¤æ‹¬å·å†…çš„æ³¨é‡Šåæ¯”è¾ƒï¼‰
        import re
        # ç§»é™¤æ‰€æœ‰æ‹¬å·å†…å®¹ï¼ˆåŒ…æ‹¬ä¸­è‹±æ–‡æ‹¬å·ï¼‰
        clean1_no_paren = re.sub(r'[(ï¼ˆ][^)ï¼‰]*[)ï¼‰]', '', clean1)
        clean2_no_paren = re.sub(r'[(ï¼ˆ][^)ï¼‰]*[)ï¼‰]', '', clean2)
        
        # å¦‚æœç§»é™¤æ‹¬å·åå†…å®¹ç›¸åŒï¼Œè®¤ä¸ºæ˜¯é‡å 
        if clean1_no_paren and clean2_no_paren and clean1_no_paren == clean2_no_paren:
            return True
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        shorter_len = min(len(clean1), len(clean2))
        longer_len = max(len(clean1), len(clean2))
        
        # å¦‚æœè¾ƒçŸ­æ–‡æœ¬å®Œå…¨åŒ…å«åœ¨è¾ƒé•¿æ–‡æœ¬ä¸­ï¼Œè®¤ä¸ºæ˜¯é‡å 
        if clean1 in clean2 or clean2 in clean1:
            # ä½†éœ€è¦æ£€æŸ¥é•¿åº¦æ¯”ä¾‹ï¼Œé¿å…è¯¯åˆ¤
            if shorter_len / longer_len >= 0.70:  # é™ä½åˆ°70%ï¼Œå› ä¸ºæ‹¬å·æ³¨é‡Šä¼šå¢åŠ é•¿åº¦
                return True
        
        # æ£€æŸ¥ç§»é™¤æ‹¬å·åçš„åŒ…å«å…³ç³»
        if clean1_no_paren and clean2_no_paren:
            shorter_no_paren = min(len(clean1_no_paren), len(clean2_no_paren))
            longer_no_paren = max(len(clean1_no_paren), len(clean2_no_paren))
            if clean1_no_paren in clean2_no_paren or clean2_no_paren in clean1_no_paren:
                if shorter_no_paren / longer_no_paren >= 0.75:
                    return True
        
        # ä½¿ç”¨ç®€å•çš„å­—ç¬¦åŒ¹é…ç›¸ä¼¼åº¦
        common_chars = sum(1 for c1, c2 in zip(clean1, clean2) if c1 == c2)
        similarity = common_chars / shorter_len
        
        # ç›¸ä¼¼åº¦è¶…è¿‡85%è®¤ä¸ºæ˜¯é‡å ï¼ˆé™ä½é˜ˆå€¼ä»¥å¤„ç†æ”¹å†™ï¼‰
        return similarity >= 0.85
    
    def _similar_text(self, text1: str, text2: str, threshold: float = 0.8) -> bool:
        """åˆ¤æ–­ä¸¤æ®µæ–‡æœ¬æ˜¯å¦ç›¸ä¼¼
        
        Args:
            text1: æ–‡æœ¬1
            text2: æ–‡æœ¬2
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            
        Returns:
            æ˜¯å¦ç›¸ä¼¼
        """
        # ç®€å•çš„ç›¸ä¼¼åº¦è®¡ç®—ï¼šåŸºäºå­—ç¬¦åŒ¹é…
        if not text1 or not text2:
            return False
        
        # ç§»é™¤ç©ºç™½å­—ç¬¦åæ¯”è¾ƒ
        clean1 = ''.join(text1.split())
        clean2 = ''.join(text2.split())
        
        if not clean1 or not clean2:
            return False
        
        # è®¡ç®—æœ€é•¿å…¬å…±å­åºåˆ—é•¿åº¦
        shorter = min(len(clean1), len(clean2))
        longer = max(len(clean1), len(clean2))
        
        if shorter == 0:
            return False
        
        # ç®€å•åŒ¹é…ï¼šå¦‚æœè¾ƒçŸ­æ–‡æœ¬çš„å¤§éƒ¨åˆ†å‡ºç°åœ¨è¾ƒé•¿æ–‡æœ¬ä¸­
        if clean1 in clean2 or clean2 in clean1:
            return True
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        common = sum(1 for c1, c2 in zip(clean1, clean2) if c1 == c2)
        similarity = common / shorter
        
        return similarity >= threshold
    
    def _deduplicate_images(self, images: List[ImageInfo]) -> List[ImageInfo]:
        """å»é™¤é‡å¤çš„å›¾ç‰‡
        
        Args:
            images: å›¾ç‰‡åˆ—è¡¨
            
        Returns:
            å»é‡åçš„å›¾ç‰‡åˆ—è¡¨
        """
        seen_urls = set()
        unique_images = []
        
        for img in images:
            if img.url not in seen_urls:
                seen_urls.add(img.url)
                unique_images.append(img)
        
        return unique_images
    
    def _build_prompt(self, html: str) -> str:
        """æ„å»ºLLMæç¤ºè¯
        
        Args:
            html: HTMLå†…å®¹
            
        Returns:
            å®Œæ•´çš„æç¤ºè¯
        """
        # é™åˆ¶HTMLé•¿åº¦ä»¥é¿å…è¶…å‡ºtokené™åˆ¶
        # æ³¨æ„ï¼šè¿™ä¸ªé™åˆ¶ä¸»è¦ç”¨äºå•æ¬¡å¤„ç†ï¼ˆéåˆ†æ®µï¼‰ï¼Œå®é™…ä¸Šåˆ†æ®µå¤„ç†ä¼šè‡ªåŠ¨æ‹†åˆ†
        max_html_length = 2000000  # çº¦2MBï¼Œé¢„å¤„ç†åçš„HTMLåº”è¯¥å¾ˆç®€æ´
        if len(html) > max_html_length:
            logger.warning(f"HTML too long ({len(html)} chars), truncating to {max_html_length}")
            logger.warning("å»ºè®®ä½¿ç”¨åˆ†æ®µå¤„ç†æ¨¡å¼ï¼ˆè‡ªåŠ¨è§¦å‘ï¼‰æ¥å¤„ç†è¶…å¤§HTML")
            html = html[:max_html_length] + "\n... (truncated)"
        
        # æ›¿æ¢æ¨¡æ¿ä¸­çš„å ä½ç¬¦
        prompt = self.prompt_template.replace("{html}", html)
        
        return prompt
    
    def _parse_llm_response(self, response: str) -> ExtractedContent:
        """è§£æLLMè¿”å›çš„JSONå“åº”
        
        Args:
            response: LLMè¿”å›çš„JSONå­—ç¬¦ä¸²
            
        Returns:
            ExtractedContentå¯¹è±¡
            
        Raises:
            json.JSONDecodeError: JSONè§£æå¤±è´¥
        """
        import re
        
        # å°è¯•ç›´æ¥è§£æ
        try:
            data = json.loads(response)
        except json.JSONDecodeError as e:
            logger.warning(f"JSONè§£æå¤±è´¥ï¼Œå°è¯•ä¿®å¤: {e}")
            
            # å°è¯•ä¿®å¤JSONæ ¼å¼
            # 1. æå–JSONå—ï¼ˆå»é™¤å‰åçš„markdownä»£ç å—æ ‡è®°ï¼‰
            json_match = re.search(r'```(?:json)?\s*({.*?})\s*```', response, re.DOTALL)
            if json_match:
                response = json_match.group(1)
            
            # 2. å¤„ç†å¸¸è§çš„JSONé”™è¯¯
            # ç§»é™¤æœ«å°¾çš„é€—å·
            response = re.sub(r',\s*}', '}', response)
            response = re.sub(r',\s*]', ']', response)
            
            # å†æ¬¡å°è¯•è§£æ
            try:
                data = json.loads(response)
                logger.info("âœ… JSONä¿®å¤æˆåŠŸ")
            except json.JSONDecodeError as e2:
                logger.error(f"âŒ JSONä¿®å¤å¤±è´¥: {e2}")
                logger.error(f"Response preview: {response[:500]}...")
                # è¿”å›ä¸€ä¸ªç©ºçš„ç»“æœï¼Œè€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
                logger.warning("è¿”å›ç©ºå†…å®¹ï¼Œè·¨è¿‡æ­¤åˆ†æ®µ")
                return ExtractedContent(
                    title="",
                    content="",
                    images=[],
                    metadata={}
                )
        
        # æå–å­—æ®µ
        title = data.get("title", "")
        content = data.get("content", "")
        images_data = data.get("images", [])
        metadata = data.get("metadata", {})
        
        # åˆ›å»ºImageInfoå¯¹è±¡åˆ—è¡¨
        images = []
        for i, img_data in enumerate(images_data):
            try:
                image = ImageInfo(
                    url=img_data.get("url", ""),
                    alt=img_data.get("alt", ""),
                    caption=img_data.get("caption"),
                    position=i
                )
                images.append(image)
            except Exception as e:
                logger.warning(f"Failed to parse image data: {img_data}, error: {e}")
                continue
        
        # åˆ›å»ºExtractedContentå¯¹è±¡
        extracted_content = ExtractedContent(
            title=title,
            content=content,
            images=images,
            metadata=metadata
        )
        
        return extracted_content
    
    def _validate_content(self, content: ExtractedContent) -> None:
        """éªŒè¯æå–çš„å†…å®¹
        
        Args:
            content: æå–çš„å†…å®¹å¯¹è±¡
            
        Raises:
            ContentExtractionError: å†…å®¹æ— æ•ˆ
        """
        # æ£€æŸ¥æ ‡é¢˜
        if not content.title or not content.title.strip():
            logger.warning("Extracted title is empty")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå› ä¸ºæœ‰äº›é¡µé¢å¯èƒ½æ²¡æœ‰æ˜ç¡®çš„æ ‡é¢˜
        
        # æ£€æŸ¥æ­£æ–‡
        if not content.content or not content.content.strip():
            logger.error("Extracted content is empty")
            raise ContentExtractionError(
                "Failed to extract content from HTML. "
                "The page may not contain article content, or it may be behind a paywall/login."
            )
        
        # æ£€æŸ¥æ­£æ–‡é•¿åº¦
        if len(content.content.strip()) < 50:
            logger.warning(f"Extracted content is very short: {len(content.content)} chars")
            raise ContentExtractionError(
                f"Extracted content is too short ({len(content.content)} chars). "
                "The page may not contain substantial article content."
            )
        
        logger.debug("Content validation passed")
