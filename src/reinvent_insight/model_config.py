"""
ç»Ÿä¸€æ¨¡å‹é…ç½®ç³»ç»Ÿ

è¯¥æ¨¡å—æä¾›ç»Ÿä¸€çš„æ¨¡å‹é…ç½®ç®¡ç†å’Œå®¢æˆ·ç«¯æ¥å£ï¼Œæ”¯æŒæŒ‰ä»»åŠ¡ç±»å‹é…ç½®ä¸åŒçš„æ¨¡å‹å’Œå‚æ•°ã€‚
"""

import asyncio
import logging
import time
import yaml
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from pathlib import Path
from typing import Dict, Any, Optional, Type, Callable
import os

logger = logging.getLogger(__name__)


# ============================================================================
# æ•°æ®æ¨¡å‹
# ============================================================================

@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®æ•°æ®ç±»"""
    task_type: str                    # ä»»åŠ¡ç±»å‹æ ‡è¯†
    provider: str                     # æ¨¡å‹æä¾›å•† (gemini/xai/alibaba)
    model_name: str                   # å…·ä½“æ¨¡å‹åç§°
    api_key: str                      # APIå¯†é’¥
    
    # æ€è€ƒæ¨¡å¼é…ç½®
    low_thinking: bool = False        # æ˜¯å¦å¯ç”¨ä½æ€è€ƒæ¨¡å¼ï¼ˆé»˜è®¤falseï¼Œä½¿ç”¨é«˜æ€è€ƒï¼‰
    
    # ç”Ÿæˆå‚æ•°
    temperature: float = 0.7
    top_p: float = 0.9
    top_k: int = 40
    max_output_tokens: int = 8000
    
    # é€Ÿç‡é™åˆ¶
    rate_limit_interval: float = 0.5  # APIè°ƒç”¨é—´éš”ï¼ˆç§’ï¼‰
    max_retries: int = 3              # æœ€å¤§é‡è¯•æ¬¡æ•°
    retry_backoff_base: float = 2.0   # é‡è¯•é€€é¿åŸºæ•°


# ============================================================================
# å¼‚å¸¸ç±»
# ============================================================================

class ModelConfigError(Exception):
    """æ¨¡å‹é…ç½®ç›¸å…³é”™è¯¯çš„åŸºç±»"""
    pass


class ConfigurationError(ModelConfigError):
    """é…ç½®é”™è¯¯"""
    pass


class UnsupportedProviderError(ModelConfigError):
    """ä¸æ”¯æŒçš„æ¨¡å‹æä¾›å•†"""
    pass


class APIError(ModelConfigError):
    """APIè°ƒç”¨é”™è¯¯"""
    pass


# ============================================================================
# é€Ÿç‡é™åˆ¶å™¨
# ============================================================================

class RateLimiter:
    """å…¨å±€é€Ÿç‡é™åˆ¶å™¨"""
    
    _lock = asyncio.Lock()
    _last_call_time: Dict[str, float] = {}
    
    def __init__(self, interval: float):
        """
        åˆå§‹åŒ–é€Ÿç‡é™åˆ¶å™¨
        
        Args:
            interval: è°ƒç”¨é—´éš”ï¼ˆç§’ï¼‰
        """
        self.interval = interval
        
    async def acquire(self, key: str = "default") -> None:
        """
        è·å–è°ƒç”¨è®¸å¯ï¼Œå¿…è¦æ—¶ç­‰å¾…
        
        Args:
            key: é™åˆ¶å™¨é”®åï¼Œç”¨äºåŒºåˆ†ä¸åŒçš„é™åˆ¶å™¨
        """
        async with self._lock:
            now = time.monotonic()
            last_time = self._last_call_time.get(key, 0.0)
            elapsed = now - last_time
            
            if elapsed < self.interval:
                sleep_time = self.interval - elapsed
                logger.debug(f"è§¦å‘é€Ÿç‡é™åˆ¶ [{key}]ï¼Œç­‰å¾… {sleep_time:.2f} ç§’")
                await asyncio.sleep(sleep_time)
            
            self._last_call_time[key] = time.monotonic()


# ============================================================================
# æ¨¡å‹å®¢æˆ·ç«¯æŠ½è±¡åŸºç±»
# ============================================================================

class BaseModelClient(ABC):
    """æ¨¡å‹å®¢æˆ·ç«¯æŠ½è±¡åŸºç±»"""
    
    def __init__(self, config: ModelConfig):
        """
        åˆå§‹åŒ–æ¨¡å‹å®¢æˆ·ç«¯
        
        Args:
            config: æ¨¡å‹é…ç½®
        """
        self.config = config
        self._rate_limiter = RateLimiter(config.rate_limit_interval)
        
    @abstractmethod
    async def generate_content(
        self, 
        prompt: str, 
        is_json: bool = False
    ) -> str:
        """
        ç”Ÿæˆæ–‡æœ¬å†…å®¹
        
        Args:
            prompt: æç¤ºè¯
            is_json: æ˜¯å¦è¿”å›JSONæ ¼å¼
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹
            
        Raises:
            APIError: APIè°ƒç”¨å¤±è´¥
        """
        pass
        
    @abstractmethod
    async def generate_content_with_file(
        self,
        prompt: str,
        file_info: Dict[str, Any],
        is_json: bool = False
    ) -> str:
        """
        ä½¿ç”¨æ–‡ä»¶ç”Ÿæˆå†…å®¹ï¼ˆå¤šæ¨¡æ€ï¼‰
        
        Args:
            prompt: æç¤ºè¯
            file_info: æ–‡ä»¶ä¿¡æ¯å­—å…¸
            is_json: æ˜¯å¦è¿”å›JSONæ ¼å¼
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹
            
        Raises:
            APIError: APIè°ƒç”¨å¤±è´¥
        """
        pass
        
    async def _apply_rate_limit(self) -> None:
        """åº”ç”¨é€Ÿç‡é™åˆ¶"""
        await self._rate_limiter.acquire(self.config.provider)
        
    async def _retry_with_backoff(
        self,
        func: Callable,
        *args,
        **kwargs
    ) -> Any:
        """
        å¸¦æŒ‡æ•°é€€é¿çš„é‡è¯•æœºåˆ¶
        
        Args:
            func: è¦æ‰§è¡Œçš„å¼‚æ­¥å‡½æ•°
            *args: ä½ç½®å‚æ•°
            **kwargs: å…³é”®å­—å‚æ•°
            
        Returns:
            å‡½æ•°æ‰§è¡Œç»“æœ
            
        Raises:
            APIError: è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°åä»å¤±è´¥
        """
        last_exception = None
        
        for attempt in range(self.config.max_retries):
            try:
                return await func(*args, **kwargs)
            except Exception as e:
                last_exception = e
                if attempt < self.config.max_retries - 1:
                    wait_time = self.config.retry_backoff_base ** attempt
                    logger.warning(
                        f"APIè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{self.config.max_retries}): {e}"
                    )
                    logger.info(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    await asyncio.sleep(wait_time)
                else:
                    logger.error(
                        f"APIè°ƒç”¨å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•° ({self.config.max_retries})"
                    )
        
        raise APIError(f"APIè°ƒç”¨å¤±è´¥: {last_exception}") from last_exception


# ============================================================================
# é…ç½®ç®¡ç†å™¨
# ============================================================================

class ModelConfigManager:
    """æ¨¡å‹é…ç½®ç®¡ç†å™¨"""
    
    _instance: Optional['ModelConfigManager'] = None
    _configs: Dict[str, ModelConfig] = {}
    _default_config: Optional[ModelConfig] = None
    
    def __init__(self, config_path: Optional[Path] = None):
        """
        åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸º config/model_config.yaml
        """
        if config_path is None:
            # è·å–é¡¹ç›®æ ¹ç›®å½•
            from . import config as app_config
            config_path = app_config.PROJECT_ROOT / "config" / "model_config.yaml"
        
        self.config_path = Path(config_path)
        self.load_config()
        
    def load_config(self) -> None:
        """ä»é…ç½®æ–‡ä»¶å’Œç¯å¢ƒå˜é‡åŠ è½½é…ç½®"""
        try:
            if not self.config_path.exists():
                logger.warning(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {self.config_path}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                self._load_default_config()
                return
            
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config_data = yaml.safe_load(f)
            
            if not config_data:
                logger.warning("é…ç½®æ–‡ä»¶ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                self._load_default_config()
                return
            
            # åŠ è½½é»˜è®¤é…ç½®
            if 'default' in config_data:
                self._default_config = self._parse_config('default', config_data['default'])
            else:
                self._load_default_config()
            
            # åŠ è½½ä»»åŠ¡ç‰¹å®šé…ç½®
            if 'tasks' in config_data:
                for task_type, task_config in config_data['tasks'].items():
                    self._configs[task_type] = self._parse_config(task_type, task_config)
            
            logger.info(f"æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶: {self.config_path}")
            logger.info(f"å·²åŠ è½½ {len(self._configs)} ä¸ªä»»åŠ¡é…ç½®")
            
        except Exception as e:
            logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}", exc_info=True)
            self._load_default_config()
    
    def _parse_config(self, task_type: str, config_dict: Dict[str, Any]) -> ModelConfig:
        """
        è§£æé…ç½®å­—å…¸ä¸º ModelConfig å¯¹è±¡
        
        Args:
            task_type: ä»»åŠ¡ç±»å‹
            config_dict: é…ç½®å­—å…¸
            
        Returns:
            ModelConfig å¯¹è±¡
        """
        # è·å– API Key
        api_key_env = config_dict.get('api_key_env', 'GEMINI_API_KEY')
        api_key = os.getenv(api_key_env, '')
        
        # æ£€æŸ¥ç¯å¢ƒå˜é‡è¦†ç›–
        provider = self._get_env_override(task_type, 'provider', config_dict.get('provider', 'gemini'))
        model_name = self._get_env_override(task_type, 'model_name', config_dict.get('model_name', 'gemini-2.0-flash-exp'))
        
        # æ€è€ƒæ¨¡å¼é…ç½®
        thinking = config_dict.get('thinking', {})
        low_thinking = bool(self._get_env_override(task_type, 'low_thinking', thinking.get('low_thinking', False)))
        
        # ç”Ÿæˆå‚æ•°
        generation = config_dict.get('generation', {})
        temperature = float(self._get_env_override(task_type, 'temperature', generation.get('temperature', 0.7)))
        top_p = float(self._get_env_override(task_type, 'top_p', generation.get('top_p', 0.9)))
        top_k = int(self._get_env_override(task_type, 'top_k', generation.get('top_k', 40)))
        max_output_tokens = int(self._get_env_override(task_type, 'max_output_tokens', generation.get('max_output_tokens', 8000)))
        
        # é€Ÿç‡é™åˆ¶å‚æ•°
        rate_limit = config_dict.get('rate_limit', {})
        rate_limit_interval = float(self._get_env_override(task_type, 'rate_limit_interval', rate_limit.get('interval', 0.5)))
        max_retries = int(self._get_env_override(task_type, 'max_retries', rate_limit.get('max_retries', 3)))
        retry_backoff_base = float(self._get_env_override(task_type, 'retry_backoff_base', rate_limit.get('retry_backoff_base', 2.0)))
        
        return ModelConfig(
            task_type=task_type,
            provider=provider,
            model_name=model_name,
            api_key=api_key,
            low_thinking=low_thinking,
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            max_output_tokens=max_output_tokens,
            rate_limit_interval=rate_limit_interval,
            max_retries=max_retries,
            retry_backoff_base=retry_backoff_base
        )
    
    def _get_env_override(self, task_type: str, param_name: str, default_value: Any) -> Any:
        """
        è·å–ç¯å¢ƒå˜é‡è¦†ç›–å€¼
        
        ç¯å¢ƒå˜é‡å‘½åè§„èŒƒ: MODEL_{TASK_TYPE}_{PARAMETER}
        ä¾‹å¦‚: MODEL_PDF_PROCESSING_MODEL_NAME
        
        Args:
            task_type: ä»»åŠ¡ç±»å‹
            param_name: å‚æ•°åç§°
            default_value: é»˜è®¤å€¼
            
        Returns:
            ç¯å¢ƒå˜é‡å€¼æˆ–é»˜è®¤å€¼
        """
        env_key = f"MODEL_{task_type.upper()}_{param_name.upper()}"
        env_value = os.getenv(env_key)
        
        if env_value is not None:
            logger.debug(f"ä½¿ç”¨ç¯å¢ƒå˜é‡è¦†ç›–: {env_key}={env_value}")
            return env_value
        
        return default_value
    
    def _load_default_config(self) -> None:
        """åŠ è½½ç¡¬ç¼–ç çš„é»˜è®¤é…ç½®"""
        from . import config as app_config
        
        self._default_config = ModelConfig(
            task_type='default',
            provider='gemini',
            model_name='gemini-2.0-flash-exp',
            api_key=app_config.GEMINI_API_KEY or '',
            temperature=0.7,
            top_p=0.9,
            top_k=40,
            max_output_tokens=8000,
            rate_limit_interval=0.5,
            max_retries=3,
            retry_backoff_base=2.0
        )
        
        logger.info("ä½¿ç”¨ç¡¬ç¼–ç çš„é»˜è®¤é…ç½®")
    
    def get_config(self, task_type: str) -> ModelConfig:
        """
        è·å–æŒ‡å®šä»»åŠ¡ç±»å‹çš„é…ç½®
        
        Args:
            task_type: ä»»åŠ¡ç±»å‹
            
        Returns:
            ModelConfig å¯¹è±¡
        """
        if task_type in self._configs:
            return self._configs[task_type]
        
        logger.warning(f"æœªæ‰¾åˆ°ä»»åŠ¡ç±»å‹ '{task_type}' çš„é…ç½®ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        return self.get_default_config()
    
    def get_default_config(self) -> ModelConfig:
        """
        è·å–é»˜è®¤é…ç½®
        
        Returns:
            é»˜è®¤ ModelConfig å¯¹è±¡
        """
        if self._default_config is None:
            self._load_default_config()
        
        return self._default_config
    
    def reload_config(self) -> None:
        """é‡æ–°åŠ è½½é…ç½®ï¼ˆç”¨äºçƒ­æ›´æ–°ï¼‰"""
        logger.info("é‡æ–°åŠ è½½é…ç½®...")
        self._configs.clear()
        self._default_config = None
        self.load_config()
    
    @classmethod
    def get_instance(cls, config_path: Optional[Path] = None) -> 'ModelConfigManager':
        """
        è·å–å•ä¾‹å®ä¾‹
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            
        Returns:
            ModelConfigManager å®ä¾‹
        """
        if cls._instance is None:
            cls._instance = cls(config_path)
        return cls._instance



# ============================================================================
# Gemini æ¨¡å‹å®¢æˆ·ç«¯
# ============================================================================

class GeminiClient(BaseModelClient):
    """Geminiæ¨¡å‹å®¢æˆ·ç«¯"""
    
    def __init__(self, config: ModelConfig):
        """
        åˆå§‹åŒ–Geminiå®¢æˆ·ç«¯
        
        Args:
            config: æ¨¡å‹é…ç½®
            
        Raises:
            ConfigurationError: API Keyæœªé…ç½®
        """
        super().__init__(config)
        
        if not config.api_key:
            raise ConfigurationError("Gemini API Key æœªé…ç½®")
        
        try:
            from google import genai
            from google.genai import types
            
            self.genai = genai
            self.types = types
            
            # åˆ›å»ºå®¢æˆ·ç«¯
            self.client = genai.Client(api_key=config.api_key)
            
            logger.info(f"Geminiå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ: {config.model_name}")
            
        except ImportError:
            raise ConfigurationError("google-genai åŒ…æœªå®‰è£…")
        except Exception as e:
            raise ConfigurationError(f"Geminiå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
    
    async def generate_content(
        self, 
        prompt: str, 
        is_json: bool = False,
        thinking_level: Optional[str] = None
    ) -> str:
        """
        ç”Ÿæˆæ–‡æœ¬å†…å®¹
        
        Args:
            prompt: æç¤ºè¯
            is_json: æ˜¯å¦è¿”å›JSONæ ¼å¼
            thinking_level: æ€è€ƒçº§åˆ« ("low", "medium", "high")ï¼Œå¦‚æœä¸ºNoneåˆ™æ ¹æ®é…ç½®è‡ªåŠ¨é€‰æ‹©
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹
            
        Raises:
            APIError: APIè°ƒç”¨å¤±è´¥
        """
        await self._apply_rate_limit()
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šthinking_levelï¼Œæ ¹æ®é…ç½®è‡ªåŠ¨é€‰æ‹©
        if thinking_level is None:
            thinking_level = "low" if self.config.low_thinking else "high"
        
        logger.info(f"å¼€å§‹ä½¿ç”¨ {self.config.model_name} ç”Ÿæˆå†…å®¹ (thinking_level={thinking_level}, from_config={thinking_level is None})...")
        
        # ä½¿ç”¨æ–°çš„google.genai SDK
        config = self.types.GenerateContentConfig(
            temperature=self.config.temperature,
            top_p=self.config.top_p,
            top_k=self.config.top_k,
            max_output_tokens=self.config.max_output_tokens,
            response_mime_type="application/json" if is_json else "text/plain",
            thinking_config=self.types.ThinkingConfig(thinking_level=thinking_level)
        )
        
        async def _generate():
            response = await self.client.aio.models.generate_content(
                model=self.config.model_name,
                contents=prompt,
                config=config
            )
            
            # æå–æ–‡æœ¬å†…å®¹
            if not response.text:
                raise APIError("API è¿”å›çš„å†…å®¹ä¸ºç©ºæ–‡æœ¬")
            
            return response.text
        
        try:
            content = await self._retry_with_backoff(_generate)
            logger.info(f"{self.config.model_name} å†…å®¹ç”Ÿæˆå®Œæˆ")
            return content
            
        except Exception as e:
            logger.error(f"è°ƒç”¨ Gemini API æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            if "API key not valid" in str(e):
                raise ConfigurationError("Gemini API å¯†é’¥æ— æ•ˆ")
            raise APIError(f"Gemini API è°ƒç”¨å¤±è´¥: {e}") from e
    
    async def generate_content_with_file(
        self,
        prompt: str,
        file_info: Dict[str, Any],
        is_json: bool = False,
        thinking_level: Optional[str] = None
    ) -> str:
        """
        ä½¿ç”¨æ–‡ä»¶ç”Ÿæˆå†…å®¹ï¼ˆå¤šæ¨¡æ€ï¼‰
        
        Args:
            prompt: æç¤ºè¯
            file_info: æ–‡ä»¶ä¿¡æ¯å­—å…¸ï¼ŒåŒ…å«nameã€uriã€local_fileç­‰å­—æ®µ
            is_json: æ˜¯å¦è¿”å›JSONæ ¼å¼
            thinking_level: æ€è€ƒçº§åˆ« ("low", "medium", "high")ï¼Œå¦‚æœä¸ºNoneåˆ™æ ¹æ®é…ç½®è‡ªåŠ¨é€‰æ‹©
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹
            
        Raises:
            APIError: APIè°ƒç”¨å¤±è´¥
        """
        await self._apply_rate_limit()
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šthinking_levelï¼Œæ ¹æ®é…ç½®è‡ªåŠ¨é€‰æ‹©
        if thinking_level is None:
            thinking_level = "low" if self.config.low_thinking else "high"
        
        logger.info(f"å¼€å§‹ä½¿ç”¨ {self.config.model_name} è¿›è¡Œå¤šæ¨¡æ€åˆ†æ (thinking_level={thinking_level})...")
        
        generation_config = self.genai.types.GenerationConfig(
            temperature=self.config.temperature,
            top_p=self.config.top_p,
            top_k=self.config.top_k,
            max_output_tokens=self.config.max_output_tokens,
            response_mime_type="application/json" if is_json else "text/plain",
            thinking_config=self.genai.types.ThinkingConfig(thinking_level=thinking_level)
        )
        
        async def _generate():
            # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©å¤„ç†æ–¹å¼
            if file_info.get("local_file", False):
                # ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
                file_path = file_info["uri"]
                
                # ä½¿ç”¨å¼‚æ­¥æ–¹å¼è¯»å–æ–‡ä»¶å¹¶å¤„ç†
                loop = asyncio.get_event_loop()
                
                def read_and_process():
                    with open(file_path, "rb") as f:
                        file_data = f.read()
                    
                    mime_type = file_info.get("mime_type", "application/pdf")
                    
                    return self.genai.GenerativeModel(self.config.model_name).generate_content(
                        [prompt, {"mime_type": mime_type, "data": file_data}],
                        generation_config=generation_config
                    )
                
                response = await loop.run_in_executor(None, read_and_process)
            else:
                # è·å–å·²ä¸Šä¼ çš„æ–‡ä»¶å¼•ç”¨
                file_ref = self.genai.get_file(name=file_info["name"])
                
                # è°ƒç”¨Geminiç”ŸæˆAPI
                response = await self.model.generate_content_async(
                    [prompt, file_ref],
                    generation_config=generation_config
                )
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å€™é€‰å†…å®¹
            if not response.candidates:
                raise APIError("API è¿”å›äº†ç©ºçš„å€™é€‰å†…å®¹")
            
            # æå–æ–‡æœ¬å†…å®¹
            content = ''.join(
                part.text for part in response.candidates[0].content.parts
            )
            
            if not content:
                raise APIError("API è¿”å›çš„å†…å®¹ä¸ºç©ºæ–‡æœ¬")
            
            return content
        
        try:
            content = await self._retry_with_backoff(_generate)
            logger.info(f"{self.config.model_name} å¤šæ¨¡æ€åˆ†æå®Œæˆ")
            return content
            
        except Exception as e:
            logger.error(f"è°ƒç”¨ Gemini API è¿›è¡Œå¤šæ¨¡æ€åˆ†ææ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            if "API key not valid" in str(e):
                raise ConfigurationError("Gemini API å¯†é’¥æ— æ•ˆ")
            raise APIError(f"Gemini API å¤šæ¨¡æ€è°ƒç”¨å¤±è´¥: {e}") from e
    
    async def upload_file(self, file_path: str) -> Dict[str, Any]:
        """
        ä¸Šä¼ æ–‡ä»¶åˆ°Gemini API
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ–‡ä»¶ä¿¡æ¯å­—å…¸
            
        Raises:
            APIError: ä¸Šä¼ å¤±è´¥
        """
        try:
            loop = asyncio.get_event_loop()
            
            # å°è¯•ä¸Šä¼ æ–‡ä»¶
            try:
                file_obj = await loop.run_in_executor(
                    None, 
                    lambda: self.genai.upload_file(path=file_path)
                )
                
                file_info = {
                    "name": file_obj.name,
                    "display_name": file_obj.display_name,
                    "mime_type": file_obj.mime_type,
                    "size_bytes": file_obj.size_bytes,
                    "create_time": file_obj.create_time,
                    "expiration_time": file_obj.expiration_time,
                    "uri": file_obj.uri,
                    "local_file": False
                }
                
                logger.info(f"æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {file_info['name']}")
                return file_info
                
            except TypeError as te:
                if "ragStoreName" in str(te):
                    # APIå˜æ›´ï¼Œä½¿ç”¨æœ¬åœ°æ–‡ä»¶å¤„ç†
                    logger.warning("æ£€æµ‹åˆ° API å˜æ›´ï¼Œä½¿ç”¨æœ¬åœ°æ–‡ä»¶å¤„ç†")
                    
                    import os
                    file_size = os.path.getsize(file_path)
                    
                    file_info = {
                        "name": f"files/{os.path.basename(file_path)}",
                        "display_name": os.path.basename(file_path),
                        "mime_type": "application/pdf",
                        "size_bytes": file_size,
                        "create_time": None,
                        "expiration_time": None,
                        "uri": file_path,
                        "local_file": True
                    }
                    
                    logger.info(f"ä½¿ç”¨æœ¬åœ°æ–‡ä»¶å¤„ç†: {file_info['name']}")
                    return file_info
                else:
                    raise te
                    
        except Exception as e:
            logger.error(f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {e}")
            raise APIError(f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {e}") from e
    
    async def delete_file(self, file_id: str) -> bool:
        """
        åˆ é™¤Geminiä¸Šä¼ çš„æ–‡ä»¶
        
        Args:
            file_id: æ–‡ä»¶ID
            
        Returns:
            åˆ é™¤æ˜¯å¦æˆåŠŸ
        """
        try:
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(
                None, 
                lambda: self.genai.delete_file(name=file_id)
            )
            logger.info(f"å·²åˆ é™¤æ–‡ä»¶: {file_id}")
            return True
        except Exception as e:
            logger.error(f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {e}")
            return False
    
    def _split_text_for_streaming(self, text: str, max_chunk_size: int = 100) -> list:
        """
        å°†é•¿æ–‡æœ¬æŒ‰å¥å­åˆ‡åˆ†ä¸ºå¤šä¸ªè¾ƒçŸ­çš„ç‰‡æ®µï¼Œç”¨äºæ¨¡æ‹Ÿæµå¼ä½“éªŒ
        
        ç­–ç•¥ï¼šæŒ‰å¥å­è¾¹ç•Œï¼ˆã€‚ï¼ï¼Ÿ.!? æˆ–æ¢è¡Œç¬¦ï¼‰åˆ‡åˆ†ï¼Œæ¯ä¸ªç‰‡æ®µçº¦ 50-100 å­—
        
        Args:
            text: è¦åˆ‡åˆ†çš„æ–‡æœ¬
            max_chunk_size: æ¯ä¸ªç‰‡æ®µçš„æœ€å¤§å­—ç¬¦æ•°
            
        Returns:
            æ–‡æœ¬ç‰‡æ®µåˆ—è¡¨
        """
        import re
        
        # å¦‚æœæ–‡æœ¬å¾ˆçŸ­ï¼Œç›´æ¥è¿”å›
        if len(text) <= max_chunk_size:
            return [text]
        
        # æŒ‰å¥å­è¾¹ç•Œåˆ‡åˆ†ï¼ˆä¸­è‹±æ–‡æ ‡ç‚¹ï¼‰
        # åŒ¹é…å¥å­ç»“æŸç¬¦å·ï¼šã€‚ï¼ï¼Ÿ.!? åé¢å¯èƒ½è·Ÿå¼•å·ã€æ‹¬å·ç­‰
        sentence_pattern = r'[ã€‚ï¼ï¼Ÿ.!?]+[ã€ã€"\'ï¼‰\)]*'
        
        chunks = []
        current_chunk = ""
        
        # å…ˆæŒ‰å¥å­åˆ‡åˆ†
        sentences = re.split(f'({sentence_pattern})', text)
        
        for i in range(0, len(sentences), 2):
            sentence = sentences[i]
            # å¦‚æœæœ‰æ ‡ç‚¹ç¬¦å·ï¼ŒåŠ ä¸Š
            if i + 1 < len(sentences):
                sentence += sentences[i + 1]
            
            # å¦‚æœå½“å‰å—åŠ ä¸Šè¿™ä¸ªå¥å­ä¸è¶…è¿‡é™åˆ¶ï¼Œå°±åŠ å…¥
            if len(current_chunk) + len(sentence) <= max_chunk_size:
                current_chunk += sentence
            else:
                # å¦åˆ™ï¼Œä¿å­˜å½“å‰å—ï¼Œå¼€å§‹æ–°å—
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence
        
        # æ·»åŠ æœ€åä¸€ä¸ªå—
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        # å¦‚æœæ²¡æœ‰åˆ‡åˆ†æˆåŠŸï¼ˆå¯èƒ½æ²¡æœ‰å¥å­è¾¹ç•Œï¼‰ï¼Œå¼ºåˆ¶æŒ‰å­—ç¬¦æ•°åˆ‡åˆ†
        if not chunks or (len(chunks) == 1 and len(chunks[0]) > max_chunk_size):
            logger.debug("æ²¡æœ‰æ‰¾åˆ°å¥å­è¾¹ç•Œï¼Œå¼ºåˆ¶æŒ‰å­—ç¬¦æ•°åˆ‡åˆ†")
            chunks = [text[i:i+max_chunk_size] for i in range(0, len(text), max_chunk_size)]
        
        # è¿‡æ»¤æ‰ç©ºç‰‡æ®µ
        chunks = [chunk for chunk in chunks if chunk.strip()]
        
        logger.info(f"ğŸ“„ æ–‡æœ¬åˆ‡åˆ†: {len(text)} å­—ç¬¦ â†’ {len(chunks)} ä¸ªç‰‡æ®µ")
        for i, chunk in enumerate(chunks):
            logger.debug(f"   ç‰‡æ®µ {i+1}: {len(chunk)} å­—ç¬¦")
        
        return chunks
    
    async def generate_tts_stream(
        self,
        text: str,
        voice: str = "kore",
        language: str = "zh-CN"
    ):
        """
        ç”Ÿæˆ TTS éŸ³é¢‘ï¼ˆä½¿ç”¨è¾“å…¥ç«¯åˆ†ç‰‡ç­–ç•¥å®ç°ä½å»¶è¿Ÿæµå¼æ’­æ”¾ï¼‰
        
        ç­–ç•¥ï¼š
        1. å°†é•¿æ–‡æœ¬åˆ‡åˆ†ä¸ºå¤šä¸ªçŸ­ç‰‡æ®µï¼ˆ50-100å­—ï¼‰
        2. ä¸²è¡Œè¯·æ±‚æ¯ä¸ªç‰‡æ®µçš„ TTS
        3. ä¸€æ—¦æ”¶åˆ°ç¬¬ä¸€ä¸ªç‰‡æ®µçš„éŸ³é¢‘å°±ç«‹å³ yield
        4. æŒç»­å¤„ç†åç»­ç‰‡æ®µï¼Œå®ç°è¿ç»­æ’­æ”¾
        
        è¿™æ ·å¯ä»¥å°†é¦–å­—å»¶è¿Ÿä» 15 ç§’é™ä½åˆ° 3-5 ç§’ï¼
        
        Args:
            text: è¦è½¬æ¢çš„æ–‡æœ¬
            voice: éŸ³è‰²åç§°ï¼ˆ30ç§å¯é€‰ï¼Œå¦‚ kore, puck, aoede ç­‰ï¼Œå…¨å°å†™ï¼‰
            language: è¯­è¨€ä»£ç ï¼ˆå¦‚ zh-CN, en-US ç­‰ï¼ŒGemini ä¼šè‡ªåŠ¨æ£€æµ‹ï¼‰
            
        Yields:
            bytes: Base64 ç¼–ç çš„ PCM éŸ³é¢‘æ•°æ®
            
        Raises:
            APIError: API è°ƒç”¨å¤±è´¥
        """
        await self._apply_rate_limit()
        
        try:
            # ä½¿ç”¨æ–°çš„ google-genai SDK
            try:
                from google import genai
                from google.genai import types
                import base64
            except ImportError:
                logger.warning("google-genai SDK æœªå®‰è£…ï¼ŒTTS åŠŸèƒ½ä¸å¯ç”¨")
                raise ConfigurationError(
                    "Gemini TTS éœ€è¦ google-genai SDKã€‚è¯·å®‰è£…: pip install google-genai"
                )
            
            # è®°å½•æµå¼€å§‹çš„è¯¦ç»†ä¿¡æ¯
            start_time = asyncio.get_event_loop().time()
            logger.info(
                f"ğŸ¤ å¼€å§‹è¾“å…¥ç«¯åˆ†ç‰‡æµå¼ TTS: "
                f"model={self.config.model_name}, "
                f"voice={voice}, "
                f"language={language}, "
                f"text_length={len(text)}"
            )
            
            # ğŸ”¥ å…³é”®ç­–ç•¥ï¼šå°†é•¿æ–‡æœ¬åˆ‡åˆ†ä¸ºå¤šä¸ªçŸ­ç‰‡æ®µï¼ˆ20-30å­—ï¼Œçº¦2-4ç§’éŸ³é¢‘ï¼‰
            text_chunks = self._split_text_for_streaming(text, max_chunk_size=30)
            logger.info(f"âœ‚ï¸  æ–‡æœ¬å·²åˆ‡åˆ†ä¸º {len(text_chunks)} ä¸ªç‰‡æ®µ")
            
            # åˆ›å»ºå®¢æˆ·ç«¯ï¼ˆåœ¨ä¸»çº¿ç¨‹ä¸­ï¼Œé¿å…çº¿ç¨‹å®‰å…¨é—®é¢˜ï¼‰
            try:
                client = genai.Client(api_key=self.config.api_key)
                logger.debug("Gemini å®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
            except Exception as e:
                logger.error(f"åˆ›å»º Gemini å®¢æˆ·ç«¯å¤±è´¥: {e}", exc_info=True)
                raise ConfigurationError(f"æ— æ³•åˆ›å»º Gemini å®¢æˆ·ç«¯: {e}") from e
            
            # å¤„ç†æ¯ä¸ªæ–‡æœ¬ç‰‡æ®µ
            total_chunk_count = 0
            total_bytes = 0
            first_audio_time = None
            
            for segment_index, text_segment in enumerate(text_chunks, 1):
                segment_start_time = asyncio.get_event_loop().time()
                logger.info(f"ğŸ¯ å¤„ç†ç‰‡æ®µ {segment_index}/{len(text_chunks)}: {len(text_segment)} å­—ç¬¦")
                
                # ä¸ºæ¯ä¸ªç‰‡æ®µç”ŸæˆéŸ³é¢‘
                async def generate_segment_audio(segment_text):
                    """ä¸ºå•ä¸ªæ–‡æœ¬ç‰‡æ®µç”ŸæˆéŸ³é¢‘"""
                    def _get_stream():
                        return client.models.generate_content_stream(
                            model=self.config.model_name,
                            contents=segment_text,
                            config=types.GenerateContentConfig(
                                response_modalities=["AUDIO"],
                                speech_config=types.SpeechConfig(
                                    voice_config=types.VoiceConfig(
                                        prebuilt_voice_config=types.PrebuiltVoiceConfig(
                                            voice_name=voice
                                        )
                                    )
                                )
                            )
                        )
                    
                    # åœ¨çº¿ç¨‹æ± ä¸­è·å–æµ
                    stream = await asyncio.to_thread(_get_stream)
                    
                    # æ”¶é›†è¿™ä¸ªç‰‡æ®µçš„æ‰€æœ‰éŸ³é¢‘å—
                    segment_audio_chunks = []
                    chunk_index = 0
                    
                    while True:
                        try:
                            chunk = await asyncio.to_thread(lambda: next(stream, None))
                            if chunk is None:
                                break
                            
                            chunk_index += 1
                            
                            # è§£æéŸ³é¢‘æ•°æ®
                            if hasattr(chunk, 'candidates') and chunk.candidates:
                                candidate = chunk.candidates[0]
                                if hasattr(candidate, 'content') and candidate.content:
                                    if hasattr(candidate.content, 'parts') and candidate.content.parts:
                                        for part in candidate.content.parts:
                                            if hasattr(part, 'inline_data') and part.inline_data:
                                                if hasattr(part.inline_data, 'data') and part.inline_data.data:
                                                    audio_data = part.inline_data.data
                                                    
                                                    # è§£ç éŸ³é¢‘æ•°æ®
                                                    if isinstance(audio_data, str):
                                                        pcm_data = base64.b64decode(audio_data)
                                                    else:
                                                        pcm_data = audio_data
                                                    
                                                    if pcm_data:
                                                        segment_audio_chunks.append(pcm_data)
                        
                        except StopIteration:
                            break
                        except Exception as e:
                            logger.error(f"è¯»å–ç‰‡æ®µéŸ³é¢‘å—æ—¶å‡ºé”™: {e}")
                            break
                    
                    return segment_audio_chunks
                
                # ç”Ÿæˆè¿™ä¸ªç‰‡æ®µçš„éŸ³é¢‘
                segment_audio_chunks = await generate_segment_audio(text_segment)
                
                if not segment_audio_chunks:
                    logger.warning(f"âš ï¸  ç‰‡æ®µ {segment_index} æ²¡æœ‰ç”ŸæˆéŸ³é¢‘ï¼Œè·³è¿‡")
                    continue
                
                # è®°å½•é¦–ä¸ªéŸ³é¢‘ç‰‡æ®µçš„å»¶è¿Ÿ
                if first_audio_time is None:
                    first_audio_time = asyncio.get_event_loop().time()
                    first_audio_latency = first_audio_time - start_time
                    logger.info(f"âš¡ é¦–ä¸ªéŸ³é¢‘ç‰‡æ®µå»¶è¿Ÿ: {first_audio_latency:.2f}s ï¼ˆç›®æ ‡ < 5sï¼‰")
                
                # ç«‹å³ yield è¿™ä¸ªç‰‡æ®µçš„æ‰€æœ‰éŸ³é¢‘å—
                for pcm_data in segment_audio_chunks:
                    b64_data = base64.b64encode(pcm_data).decode('utf-8')
                    total_chunk_count += 1
                    total_bytes += len(pcm_data)
                    
                    logger.info(
                        f"ğŸ“¦ å‘é€ç‰‡æ®µ {segment_index} çš„éŸ³é¢‘: "
                        f"{len(pcm_data)} bytes, "
                        f"ç´¯è®¡ {total_bytes / 1024:.1f}KB"
                    )
                    
                    # âœ… ç«‹å³ yield ç»™å‰ç«¯æ’­æ”¾ï¼
                    yield b64_data.encode('utf-8')
                
                segment_time = asyncio.get_event_loop().time() - segment_start_time
                logger.info(f"âœ… ç‰‡æ®µ {segment_index} å®Œæˆï¼Œè€—æ—¶ {segment_time:.2f}s")
            
            # è®°å½•å®Œæˆç»Ÿè®¡
            end_time = asyncio.get_event_loop().time()
            total_time = end_time - start_time
            
            if total_chunk_count == 0:
                logger.warning("âš ï¸  æµå¼ TTS å®Œæˆä½†æ²¡æœ‰ç”Ÿæˆä»»ä½•éŸ³é¢‘")
            else:
                avg_chunk_size = total_bytes / total_chunk_count if total_chunk_count > 0 else 0
                logger.info(
                    f"ğŸ‰ è¾“å…¥ç«¯åˆ†ç‰‡æµå¼ TTS å®Œæˆ: "
                    f"{len(text_chunks)} ä¸ªæ–‡æœ¬ç‰‡æ®µ, "
                    f"{total_chunk_count} ä¸ªéŸ³é¢‘å—, "
                    f"{total_bytes / 1024:.1f}KB, "
                    f"å¹³å‡å—å¤§å° {avg_chunk_size / 1024:.1f}KB, "
                    f"æ€»æ—¶é•¿ {total_time:.2f}s"
                )
                
                if first_audio_time:
                    first_audio_latency = first_audio_time - start_time
                    logger.info(f"ğŸ“Š æ€§èƒ½æŒ‡æ ‡: é¦–éŸ³é¢‘å»¶è¿Ÿ {first_audio_latency:.2f}s")
                    
                    if first_audio_latency < 5.0:
                        logger.info("ğŸ¯ æˆåŠŸï¼é¦–éŸ³é¢‘å»¶è¿Ÿ < 5 ç§’")
                    elif first_audio_latency < 10.0:
                        logger.info("âœ… è‰¯å¥½ï¼é¦–éŸ³é¢‘å»¶è¿Ÿ < 10 ç§’")
                    else:
                        logger.warning(f"âš ï¸  é¦–éŸ³é¢‘å»¶è¿Ÿè¾ƒé•¿: {first_audio_latency:.2f}s")
            
        except ConfigurationError:
            # é…ç½®é”™è¯¯ç›´æ¥æŠ›å‡º
            raise
        except Exception as e:
            # è®°å½•è¯¦ç»†çš„é”™è¯¯ä¸Šä¸‹æ–‡
            error_context = {
                'model': self.config.model_name,
                'voice': voice,
                'language': language,
                'text_length': len(text),
                'error_type': type(e).__name__,
                'error_message': str(e)
            }
            
            logger.error(
                f"âŒ æµå¼ TTS å¤±è´¥: {e}\n"
                f"   ä¸Šä¸‹æ–‡: {error_context}",
                exc_info=True
            )
            
            # æ ¹æ®é”™è¯¯ç±»å‹æä¾›æ›´å‹å¥½çš„é”™è¯¯æ¶ˆæ¯
            error_str = str(e).lower()
            
            if "api key not valid" in error_str or "invalid api key" in error_str:
                raise ConfigurationError(
                    f"Gemini API å¯†é’¥æ— æ•ˆã€‚è¯·æ£€æŸ¥ GEMINI_API_KEY ç¯å¢ƒå˜é‡ã€‚"
                ) from e
            
            if "voice name" in error_str and "not supported" in error_str:
                raise APIError(
                    f"ä¸æ”¯æŒçš„éŸ³è‰² '{voice}'ã€‚"
                    f"è¯·ä½¿ç”¨æ”¯æŒçš„éŸ³è‰²ï¼ˆå¦‚ kore, puck, aoede ç­‰ï¼‰ã€‚"
                ) from e
            
            if "quota" in error_str or "rate limit" in error_str:
                raise APIError(
                    f"API é…é¢å·²ç”¨å°½æˆ–é€Ÿç‡é™åˆ¶ã€‚è¯·ç¨åé‡è¯•ã€‚"
                ) from e
            
            if "client has been closed" in error_str:
                raise APIError(
                    f"API å®¢æˆ·ç«¯è¿æ¥å·²å…³é—­ã€‚è¿™å¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜æˆ–è¶…æ—¶ã€‚"
                ) from e
            
            # é€šç”¨é”™è¯¯
            raise APIError(
                f"Gemini TTS æµå¼ç”Ÿæˆå¤±è´¥: {e}\n"
                f"æ¨¡å‹: {self.config.model_name}, éŸ³è‰²: {voice}"
            ) from e
    
    async def generate_tts(
        self,
        text: str,
        voice: str = "Kore",
        language: str = "zh-CN"
    ) -> bytes:
        """
        ç”Ÿæˆå®Œæ•´çš„ TTS éŸ³é¢‘ï¼ˆéæµå¼ï¼‰
        
        Args:
            text: è¦è½¬æ¢çš„æ–‡æœ¬
            voice: éŸ³è‰²åç§°
            language: è¯­è¨€ä»£ç 
            
        Returns:
            Base64 ç¼–ç çš„ PCM éŸ³é¢‘æ•°æ®
            
        Raises:
            APIError: API è°ƒç”¨å¤±è´¥
        """
        audio_data = None
        async for chunk in self.generate_tts_stream(text, voice, language):
            audio_data = chunk
            break  # Gemini TTS è¿”å›å®Œæ•´éŸ³é¢‘ï¼Œåªæœ‰ä¸€ä¸ªå—
        
        if not audio_data:
            raise APIError("æœªæ”¶åˆ°éŸ³é¢‘æ•°æ®")
        
        return audio_data



# ============================================================================
# DashScope (é˜¿é‡Œäº‘é€šä¹‰åƒé—®) æ¨¡å‹å®¢æˆ·ç«¯
# ============================================================================

class DashScopeClient(BaseModelClient):
    """DashScope (é˜¿é‡Œäº‘é€šä¹‰åƒé—®) æ¨¡å‹å®¢æˆ·ç«¯"""
    
    def __init__(self, config: ModelConfig):
        """
        åˆå§‹åŒ–DashScopeå®¢æˆ·ç«¯
        
        Args:
            config: æ¨¡å‹é…ç½®
            
        Raises:
            ConfigurationError: API Keyæœªé…ç½®æˆ–SDKæœªå®‰è£…
        """
        super().__init__(config)
        
        if not config.api_key:
            raise ConfigurationError("DashScope API Key æœªé…ç½®")
        
        try:
            from dashscope import Generation
            import dashscope
            self.dashscope = dashscope
            self.Generation = Generation
            
            # é…ç½®API Key
            dashscope.api_key = config.api_key
            
            logger.info(f"DashScopeå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ: {config.model_name}")
            
        except ImportError:
            raise ConfigurationError(
                "dashscope åŒ…æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install dashscope"
            )
        except Exception as e:
            raise ConfigurationError(f"DashScopeå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
    
    async def generate_content(
        self, 
        prompt: str, 
        is_json: bool = False
    ) -> str:
        """
        ç”Ÿæˆæ–‡æœ¬å†…å®¹
        
        Args:
            prompt: æç¤ºè¯
            is_json: æ˜¯å¦è¿”å›JSONæ ¼å¼
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹
            
        Raises:
            APIError: APIè°ƒç”¨å¤±è´¥
        """
        await self._apply_rate_limit()
        
        logger.info(f"å¼€å§‹ä½¿ç”¨ {self.config.model_name} ç”Ÿæˆå†…å®¹...")
        
        # æ„å»ºæ¶ˆæ¯
        messages = [
            {'role': 'user', 'content': prompt}
        ]
        
        # å¦‚æœéœ€è¦JSONæ ¼å¼ï¼Œåœ¨promptä¸­æ·»åŠ æŒ‡ç¤º
        if is_json:
            messages[0]['content'] = f"{prompt}\n\nè¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœã€‚"
        
        async def _generate():
            # DashScope SDK ä½¿ç”¨åŒæ­¥è°ƒç”¨ï¼Œéœ€è¦åœ¨executorä¸­è¿è¡Œ
            loop = asyncio.get_event_loop()
            
            def _call_api():
                response = self.Generation.call(
                    model=self.config.model_name,
                    messages=messages,
                    result_format='message',
                    temperature=self.config.temperature,
                    top_p=self.config.top_p,
                    max_tokens=self.config.max_output_tokens,
                )
                return response
            
            response = await loop.run_in_executor(None, _call_api)
            
            # æ£€æŸ¥å“åº”çŠ¶æ€
            if response.status_code != 200:
                raise APIError(
                    f"DashScope API è¿”å›é”™è¯¯: {response.code} - {response.message}"
                )
            
            # æå–å†…å®¹
            if not response.output or not response.output.choices:
                raise APIError("DashScope API è¿”å›äº†ç©ºçš„å†…å®¹")
            
            content = response.output.choices[0].message.content
            
            if not content:
                raise APIError("DashScope API è¿”å›çš„å†…å®¹ä¸ºç©ºæ–‡æœ¬")
            
            return content
        
        try:
            content = await self._retry_with_backoff(_generate)
            logger.info(f"{self.config.model_name} å†…å®¹ç”Ÿæˆå®Œæˆ")
            return content
            
        except Exception as e:
            logger.error(f"è°ƒç”¨ DashScope API æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            if "Invalid API-key" in str(e) or "Unauthorized" in str(e):
                raise ConfigurationError("DashScope API å¯†é’¥æ— æ•ˆ")
            raise APIError(f"DashScope API è°ƒç”¨å¤±è´¥: {e}") from e
    
    async def generate_content_with_file(
        self,
        prompt: str,
        file_info: Dict[str, Any],
        is_json: bool = False
    ) -> str:
        """
        ä½¿ç”¨æ–‡ä»¶ç”Ÿæˆå†…å®¹ï¼ˆå¤šæ¨¡æ€ï¼‰
        
        DashScope æ”¯æŒå¤šæ¨¡æ€è¾“å…¥ï¼ŒåŒ…æ‹¬å›¾ç‰‡å’Œæ–‡æ¡£
        
        Args:
            prompt: æç¤ºè¯
            file_info: æ–‡ä»¶ä¿¡æ¯å­—å…¸
            is_json: æ˜¯å¦è¿”å›JSONæ ¼å¼
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹
            
        Raises:
            APIError: APIè°ƒç”¨å¤±è´¥
        """
        await self._apply_rate_limit()
        
        logger.info(f"å¼€å§‹ä½¿ç”¨ {self.config.model_name} è¿›è¡Œå¤šæ¨¡æ€åˆ†æ...")
        
        # æ„å»ºå¤šæ¨¡æ€æ¶ˆæ¯
        content_parts = []
        
        # æ·»åŠ æ–‡æœ¬éƒ¨åˆ†
        if is_json:
            content_parts.append({
                'text': f"{prompt}\n\nè¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœã€‚"
            })
        else:
            content_parts.append({'text': prompt})
        
        # æ·»åŠ æ–‡ä»¶éƒ¨åˆ†
        if file_info.get("local_file", False):
            # æœ¬åœ°æ–‡ä»¶ï¼šè¯»å–å¹¶è½¬æ¢ä¸ºbase64
            import base64
            file_path = file_info["uri"]
            
            with open(file_path, "rb") as f:
                file_data = f.read()
            
            file_base64 = base64.b64encode(file_data).decode('utf-8')
            mime_type = file_info.get("mime_type", "application/pdf")
            
            # DashScope å¤šæ¨¡æ€æ ¼å¼
            content_parts.append({
                'file': f"data:{mime_type};base64,{file_base64}"
            })
        else:
            # è¿œç¨‹æ–‡ä»¶URL
            content_parts.append({
                'file': file_info.get("uri", "")
            })
        
        messages = [
            {
                'role': 'user',
                'content': content_parts
            }
        ]
        
        async def _generate():
            loop = asyncio.get_event_loop()
            
            def _call_api():
                # ä½¿ç”¨æ”¯æŒå¤šæ¨¡æ€çš„æ¨¡å‹
                model = self.config.model_name
                # å¦‚æœæ˜¯åŸºç¡€æ¨¡å‹ï¼Œåˆ‡æ¢åˆ°å¤šæ¨¡æ€ç‰ˆæœ¬
                if model == "qwen-turbo" or model == "qwen-plus":
                    model = "qwen-vl-plus"
                elif model == "qwen-max":
                    model = "qwen-vl-max"
                
                response = self.Generation.call(
                    model=model,
                    messages=messages,
                    result_format='message',
                    temperature=self.config.temperature,
                    top_p=self.config.top_p,
                    max_tokens=self.config.max_output_tokens,
                )
                return response
            
            response = await loop.run_in_executor(None, _call_api)
            
            # æ£€æŸ¥å“åº”çŠ¶æ€
            if response.status_code != 200:
                raise APIError(
                    f"DashScope API è¿”å›é”™è¯¯: {response.code} - {response.message}"
                )
            
            # æå–å†…å®¹
            if not response.output or not response.output.choices:
                raise APIError("DashScope API è¿”å›äº†ç©ºçš„å†…å®¹")
            
            content = response.output.choices[0].message.content
            
            if not content:
                raise APIError("DashScope API è¿”å›çš„å†…å®¹ä¸ºç©ºæ–‡æœ¬")
            
            return content
        
        try:
            content = await self._retry_with_backoff(_generate)
            logger.info(f"{self.config.model_name} å¤šæ¨¡æ€åˆ†æå®Œæˆ")
            return content
            
        except Exception as e:
            logger.error(f"è°ƒç”¨ DashScope API è¿›è¡Œå¤šæ¨¡æ€åˆ†ææ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            if "Invalid API-key" in str(e) or "Unauthorized" in str(e):
                raise ConfigurationError("DashScope API å¯†é’¥æ— æ•ˆ")
            raise APIError(f"DashScope API å¤šæ¨¡æ€è°ƒç”¨å¤±è´¥: {e}") from e
    
    async def upload_file(self, file_path: str) -> Dict[str, Any]:
        """
        DashScope ä¸éœ€è¦é¢„å…ˆä¸Šä¼ æ–‡ä»¶ï¼Œç›´æ¥åœ¨è¯·æ±‚ä¸­å‘é€
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ–‡ä»¶ä¿¡æ¯å­—å…¸
        """
        import os
        
        file_info = {
            "name": f"local/{os.path.basename(file_path)}",
            "display_name": os.path.basename(file_path),
            "mime_type": "application/pdf",
            "size_bytes": os.path.getsize(file_path),
            "create_time": None,
            "expiration_time": None,
            "uri": file_path,
            "local_file": True
        }
        
        logger.info(f"DashScope ä½¿ç”¨æœ¬åœ°æ–‡ä»¶: {file_info['name']}")
        return file_info
    
    async def delete_file(self, file_id: str) -> bool:
        """
        DashScope ä¸éœ€è¦åˆ é™¤æ–‡ä»¶ï¼ˆæ²¡æœ‰é¢„ä¸Šä¼ ï¼‰
        
        Args:
            file_id: æ–‡ä»¶ID
            
        Returns:
            æ€»æ˜¯è¿”å› True
        """
        logger.info(f"DashScope ä¸éœ€è¦åˆ é™¤æ–‡ä»¶: {file_id}")
        return True
    
    def _split_text_for_streaming(self, text: str, max_chunk_size: int = 100) -> list:
        """
        å°†é•¿æ–‡æœ¬æŒ‰å¥å­åˆ‡åˆ†ä¸ºå¤šä¸ªè¾ƒçŸ­çš„ç‰‡æ®µï¼Œç”¨äºæµå¼ä½“éªŒ
        
        ç­–ç•¥ï¼šæŒ‰å¥å­è¾¹ç•Œï¼ˆã€‚ï¼ï¼Ÿ.!? æˆ–æ¢è¡Œç¬¦ï¼‰åˆ‡åˆ†ï¼Œæ¯ä¸ªç‰‡æ®µçº¦ 50-100 å­—
        
        Args:
            text: è¦åˆ‡åˆ†çš„æ–‡æœ¬
            max_chunk_size: æ¯ä¸ªç‰‡æ®µçš„æœ€å¤§å­—ç¬¦æ•°
            
        Returns:
            æ–‡æœ¬ç‰‡æ®µåˆ—è¡¨
        """
        import re
        
        # å¦‚æœæ–‡æœ¬å¾ˆçŸ­ï¼Œç›´æ¥è¿”å›
        if len(text) <= max_chunk_size:
            return [text]
        
        # æŒ‰å¥å­è¾¹ç•Œåˆ‡åˆ†ï¼ˆä¸­è‹±æ–‡æ ‡ç‚¹ï¼‰
        sentence_pattern = r'[ã€‚ï¼ï¼Ÿ.!?]+[ã€ã€"\'ï¼‰\)]*'
        
        chunks = []
        current_chunk = ""
        
        # å…ˆæŒ‰å¥å­åˆ‡åˆ†
        sentences = re.split(f'({sentence_pattern})', text)
        
        for i in range(0, len(sentences), 2):
            sentence = sentences[i]
            # å¦‚æœæœ‰æ ‡ç‚¹ç¬¦å·ï¼ŒåŠ ä¸Š
            if i + 1 < len(sentences):
                sentence += sentences[i + 1]
            
            # å¦‚æœå½“å‰å—åŠ ä¸Šè¿™ä¸ªå¥å­ä¸è¶…è¿‡é™åˆ¶ï¼Œå°±åŠ å…¥
            if len(current_chunk) + len(sentence) <= max_chunk_size:
                current_chunk += sentence
            else:
                # å¦åˆ™ï¼Œä¿å­˜å½“å‰å—ï¼Œå¼€å§‹æ–°å—
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence
        
        # æ·»åŠ æœ€åä¸€ä¸ªå—
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        # å¦‚æœæ²¡æœ‰åˆ‡åˆ†æˆåŠŸï¼ˆå¯èƒ½æ²¡æœ‰å¥å­è¾¹ç•Œï¼‰ï¼Œå¼ºåˆ¶æŒ‰å­—ç¬¦æ•°åˆ‡åˆ†
        if not chunks or (len(chunks) == 1 and len(chunks[0]) > max_chunk_size):
            logger.debug("æ²¡æœ‰æ‰¾åˆ°å¥å­è¾¹ç•Œï¼Œå¼ºåˆ¶æŒ‰å­—ç¬¦æ•°åˆ‡åˆ†")
            chunks = [text[i:i+max_chunk_size] for i in range(0, len(text), max_chunk_size)]
        
        # è¿‡æ»¤æ‰ç©ºç‰‡æ®µ
        chunks = [chunk for chunk in chunks if chunk.strip()]
        
        logger.info(f"ğŸ“„ æ–‡æœ¬åˆ‡åˆ†: {len(text)} å­—ç¬¦ â†’ {len(chunks)} ä¸ªç‰‡æ®µ")
        for i, chunk in enumerate(chunks):
            logger.debug(f"   ç‰‡æ®µ {i+1}: {len(chunk)} å­—ç¬¦")
        
        return chunks
    
    async def generate_tts_stream(
        self,
        text: str,
        voice: str = "Cherry",
        language: str = "Chinese"
    ):
        """
        ç”Ÿæˆ TTS éŸ³é¢‘ï¼ˆä½¿ç”¨ MultiModalConversation API + è¾“å…¥ç«¯åˆ†ç‰‡ç­–ç•¥ï¼‰
        
        ä½¿ç”¨è¾“å…¥ç«¯åˆ†ç‰‡ç­–ç•¥å®ç°ä½å»¶è¿Ÿæµå¼æ’­æ”¾ï¼š
        1. å°†é•¿æ–‡æœ¬åˆ‡åˆ†ä¸ºå¤šä¸ªçŸ­ç‰‡æ®µï¼ˆ50-100å­—ï¼‰
        2. ä½¿ç”¨ MultiModalConversation.call ä¸²è¡Œå¤„ç†æ¯ä¸ªç‰‡æ®µ
        3. ä¸€æ—¦æ”¶åˆ°éŸ³é¢‘æ•°æ®å°±ç«‹å³ yield
        4. æŒç»­å¤„ç†åç»­ç‰‡æ®µï¼Œå®ç°è¿ç»­æ’­æ”¾
        
        Args:
            text: è¦è½¬æ¢çš„æ–‡æœ¬
            voice: éŸ³è‰²åç§°
            language: è¯­è¨€ç±»å‹ï¼ˆChinese, English ç­‰ï¼‰
            
        Yields:
            bytes: Base64 ç¼–ç çš„ PCM éŸ³é¢‘æ•°æ®
            
        Raises:
            APIError: API è°ƒç”¨å¤±è´¥
        """
        await self._apply_rate_limit()
        
        try:
            import base64
            
            # è®°å½•æµå¼€å§‹çš„è¯¦ç»†ä¿¡æ¯
            start_time = asyncio.get_event_loop().time()
            logger.info(
                f"ğŸ¤ å¼€å§‹ Qwen3-TTS æµå¼ TTS: "
                f"model={self.config.model_name}, "
                f"voice={voice}, "
                f"language={language}, "
                f"text_length={len(text)}"
            )
            
            # ğŸ”¥ å…³é”®ç­–ç•¥ï¼šå°†é•¿æ–‡æœ¬åˆ‡åˆ†ä¸ºå¤šä¸ªçŸ­ç‰‡æ®µï¼ˆ50-100å­—ï¼‰
            text_chunks = self._split_text_for_streaming(text, max_chunk_size=100)
            logger.info(f"âœ‚ï¸  æ–‡æœ¬å·²åˆ‡åˆ†ä¸º {len(text_chunks)} ä¸ªç‰‡æ®µ")
            
            # å¤„ç†æ¯ä¸ªæ–‡æœ¬ç‰‡æ®µ
            total_chunk_count = 0
            total_bytes = 0
            first_audio_time = None
            
            for segment_index, text_segment in enumerate(text_chunks, 1):
                segment_start_time = asyncio.get_event_loop().time()
                logger.info(f"ğŸ¯ å¤„ç†ç‰‡æ®µ {segment_index}/{len(text_chunks)}: {len(text_segment)} å­—ç¬¦")
                
                # åœ¨ç‰‡æ®µä¹‹é—´æ·»åŠ çŸ­æš‚å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«å¯¼è‡´è¿æ¥é—®é¢˜
                if segment_index > 1:
                    await asyncio.sleep(0.5)
                
                # ä¸ºæ¯ä¸ªç‰‡æ®µç”ŸæˆéŸ³é¢‘ï¼ˆä½¿ç”¨ MultiModalConversation APIï¼‰
                loop = asyncio.get_event_loop()
                
                def _call_and_collect_tts():
                    """
                    åœ¨åŒæ­¥ä¸Šä¸‹æ–‡ä¸­è°ƒç”¨ DashScope API å¹¶æ”¶é›†æ‰€æœ‰éŸ³é¢‘å—
                    
                    âš ï¸ å…³é”®ä¿®å¤ï¼šå°†æ•´ä¸ªåŒæ­¥è¿­ä»£è¿‡ç¨‹æ”¾åœ¨ executor ä¸­æ‰§è¡Œ
                    é¿å…é˜»å¡äº‹ä»¶å¾ªç¯ï¼Œç¡®ä¿æœåŠ¡å™¨ä¿æŒå“åº”
                    """
                    response = self.dashscope.MultiModalConversation.call(
                        model=self.config.model_name,
                        api_key=self.config.api_key,
                        text=text_segment,
                        voice=voice,
                        language_type=language,
                        stream=True
                    )
                    
                    # âœ… åœ¨ executor ä¸­å®ŒæˆåŒæ­¥è¿­ä»£
                    segment_audio_data = b''
                    audio_url = None
                    chunk_count = 0
                    
                    for chunk in response:  # åŒæ­¥è¿­ä»£åœ¨è¿™é‡Œå®Œæˆï¼Œä¸é˜»å¡äº‹ä»¶å¾ªç¯
                        chunk_count += 1
                        logger.debug(f"æ”¶åˆ°å“åº”å— {chunk_count}: {type(chunk)}")
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰éŸ³é¢‘æ•°æ®
                        if hasattr(chunk, 'output') and chunk.output:
                            logger.debug(f"output å­˜åœ¨: {type(chunk.output)}")
                            
                            if hasattr(chunk.output, 'audio') and chunk.output.audio:
                                audio_obj = chunk.output.audio
                                logger.debug(f"audio å¯¹è±¡: {type(audio_obj)}, data={getattr(audio_obj, 'data', None)[:50] if hasattr(audio_obj, 'data') and audio_obj.data else None}, url={getattr(audio_obj, 'url', None)}")
                                
                                # æµå¼è¾“å‡ºï¼šdata å­—æ®µåŒ…å« Base64 éŸ³é¢‘æ•°æ®
                                if hasattr(audio_obj, 'data') and audio_obj.data:
                                    audio_data = audio_obj.data
                                    # è§£ç  Base64
                                    if isinstance(audio_data, str) and audio_data:
                                        logger.info(f"æ”¶åˆ° Base64 éŸ³é¢‘æ•°æ®ï¼Œé•¿åº¦: {len(audio_data)}")
                                        audio_bytes = base64.b64decode(audio_data)
                                        segment_audio_data += audio_bytes
                                
                                # éæµå¼è¾“å‡ºï¼šurl å­—æ®µåŒ…å«å®Œæ•´éŸ³é¢‘æ–‡ä»¶ URL
                                elif hasattr(audio_obj, 'url') and audio_obj.url:
                                    audio_url = audio_obj.url
                                    logger.info(f"æ”¶åˆ°éŸ³é¢‘ URL: {audio_url}")
                            else:
                                logger.warning(f"output æ²¡æœ‰ audio å±æ€§æˆ– audio ä¸ºç©º")
                        else:
                            logger.warning(f"chunk æ²¡æœ‰ output å±æ€§æˆ– output ä¸ºç©º")
                    
                    logger.info(f"å¤„ç†äº† {chunk_count} ä¸ªå“åº”å—")
                    return segment_audio_data, audio_url
                
                # âœ… æ•´ä¸ªåŒæ­¥è¿‡ç¨‹åœ¨ executor ä¸­æ‰§è¡Œï¼Œä¸é˜»å¡äº‹ä»¶å¾ªç¯
                segment_audio_data, audio_url = await loop.run_in_executor(
                    None, _call_and_collect_tts
                )
                
                # å¦‚æœæ”¶åˆ°çš„æ˜¯ URLï¼Œéœ€è¦ä¸‹è½½éŸ³é¢‘
                if audio_url and not segment_audio_data:
                    logger.info(f"ä» URL ä¸‹è½½éŸ³é¢‘: {audio_url}")
                    import requests
                    
                    def _download_audio():
                        response = requests.get(audio_url, timeout=30)
                        response.raise_for_status()
                        return response.content
                    
                    segment_audio_data = await loop.run_in_executor(None, _download_audio)
                
                if not segment_audio_data:
                    logger.warning(f"âš ï¸  ç‰‡æ®µ {segment_index} æ²¡æœ‰ç”ŸæˆéŸ³é¢‘ï¼Œè·³è¿‡")
                    continue
                
                # è®°å½•é¦–ä¸ªéŸ³é¢‘ç‰‡æ®µçš„å»¶è¿Ÿ
                if first_audio_time is None:
                    first_audio_time = asyncio.get_event_loop().time()
                    first_audio_latency = first_audio_time - start_time
                    logger.info(f"âš¡ é¦–ä¸ªéŸ³é¢‘ç‰‡æ®µå»¶è¿Ÿ: {first_audio_latency:.2f}s ï¼ˆç›®æ ‡ < 5sï¼‰")
                
                # ç¼–ç ä¸º Base64
                b64_data = base64.b64encode(segment_audio_data).decode('utf-8')
                total_chunk_count += 1
                total_bytes += len(segment_audio_data)
                
                logger.info(
                    f"ğŸ“¦ å‘é€ç‰‡æ®µ {segment_index} çš„éŸ³é¢‘: "
                    f"{len(segment_audio_data)} bytes, "
                    f"ç´¯è®¡ {total_bytes / 1024:.1f}KB"
                )
                
                # âœ… ç«‹å³ yield ç»™å‰ç«¯æ’­æ”¾ï¼
                yield b64_data.encode('utf-8')
                
                segment_time = asyncio.get_event_loop().time() - segment_start_time
                logger.info(f"âœ… ç‰‡æ®µ {segment_index} å®Œæˆï¼Œè€—æ—¶ {segment_time:.2f}s")
            
            # è®°å½•å®Œæˆç»Ÿè®¡
            end_time = asyncio.get_event_loop().time()
            total_time = end_time - start_time
            
            if total_chunk_count == 0:
                logger.warning("âš ï¸  æµå¼ TTS å®Œæˆä½†æ²¡æœ‰ç”Ÿæˆä»»ä½•éŸ³é¢‘")
            else:
                avg_chunk_size = total_bytes / total_chunk_count if total_chunk_count > 0 else 0
                logger.info(
                    f"ğŸ‰ Qwen3-TTS æµå¼ TTS å®Œæˆ: "
                    f"{len(text_chunks)} ä¸ªæ–‡æœ¬ç‰‡æ®µ, "
                    f"{total_chunk_count} ä¸ªéŸ³é¢‘å—, "
                    f"{total_bytes / 1024:.1f}KB, "
                    f"å¹³å‡å—å¤§å° {avg_chunk_size / 1024:.1f}KB, "
                    f"æ€»æ—¶é•¿ {total_time:.2f}s"
                )
                
                if first_audio_time:
                    first_audio_latency = first_audio_time - start_time
                    logger.info(f"ğŸ“Š æ€§èƒ½æŒ‡æ ‡: é¦–éŸ³é¢‘å»¶è¿Ÿ {first_audio_latency:.2f}s")
                    
                    if first_audio_latency < 5.0:
                        logger.info("ğŸ¯ æˆåŠŸï¼é¦–éŸ³é¢‘å»¶è¿Ÿ < 5 ç§’")
                    elif first_audio_latency < 10.0:
                        logger.info("âœ… è‰¯å¥½ï¼é¦–éŸ³é¢‘å»¶è¿Ÿ < 10 ç§’")
                    else:
                        logger.warning(f"âš ï¸  é¦–éŸ³é¢‘å»¶è¿Ÿè¾ƒé•¿: {first_audio_latency:.2f}s")
            
        except Exception as e:
            logger.error(f"è°ƒç”¨ Qwen3-TTS API æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            if "Invalid API-key" in str(e) or "Unauthorized" in str(e):
                raise ConfigurationError("DashScope API å¯†é’¥æ— æ•ˆ")
            raise APIError(f"Qwen3-TTS API è°ƒç”¨å¤±è´¥: {e}") from e
    
    async def generate_tts(
        self,
        text: str,
        voice: str = "Cherry",
        language: str = "Chinese"
    ) -> bytes:
        """
        ç”Ÿæˆå®Œæ•´çš„ TTS éŸ³é¢‘ï¼ˆéæµå¼ï¼‰
        
        Args:
            text: è¦è½¬æ¢çš„æ–‡æœ¬
            voice: éŸ³è‰²åç§°
            language: è¯­è¨€ç±»å‹
            
        Returns:
            bytes: å®Œæ•´çš„éŸ³é¢‘æ•°æ®ï¼ˆBase64 ç¼–ç çš„ PCMï¼‰
            
        Raises:
            APIError: API è°ƒç”¨å¤±è´¥
        """
        logger.info(f"å¼€å§‹ä½¿ç”¨ {self.config.model_name} ç”Ÿæˆå®Œæ•´ TTS éŸ³é¢‘...")
        
        # æ”¶é›†æ‰€æœ‰éŸ³é¢‘å—
        chunks = []
        async for chunk in self.generate_tts_stream(text, voice, language):
            chunks.append(chunk)
        
        # æ‹¼æ¥æ‰€æœ‰å—
        complete_audio = b''.join(chunks)
        
        logger.info(f"TTS éŸ³é¢‘ç”Ÿæˆå®Œæˆï¼Œæ€»å¤§å°: {len(complete_audio)} bytes")
        return complete_audio


# ============================================================================
# æ¨¡å‹å®¢æˆ·ç«¯å·¥å‚
# ============================================================================

class ModelClientFactory:
    """æ¨¡å‹å®¢æˆ·ç«¯å·¥å‚"""
    
    _client_registry: Dict[str, Type[BaseModelClient]] = {
        "gemini": GeminiClient,
        "dashscope": DashScopeClient,
        # æœªæ¥å¯ä»¥æ·»åŠ æ›´å¤šæä¾›å•†
        # "xai": XAIClient,
    }
    
    @classmethod
    def create_client(cls, config: ModelConfig) -> BaseModelClient:
        """
        æ ¹æ®é…ç½®åˆ›å»ºæ¨¡å‹å®¢æˆ·ç«¯
        
        Args:
            config: æ¨¡å‹é…ç½®
            
        Returns:
            æ¨¡å‹å®¢æˆ·ç«¯å®ä¾‹
            
        Raises:
            UnsupportedProviderError: ä¸æ”¯æŒçš„æä¾›å•†
        """
        provider = config.provider.lower()
        
        if provider not in cls._client_registry:
            raise UnsupportedProviderError(
                f"ä¸æ”¯æŒçš„æ¨¡å‹æä¾›å•†: {provider}. "
                f"æ”¯æŒçš„æä¾›å•†: {', '.join(cls._client_registry.keys())}"
            )
        
        client_class = cls._client_registry[provider]
        
        try:
            return client_class(config)
        except Exception as e:
            logger.error(f"åˆ›å»ºæ¨¡å‹å®¢æˆ·ç«¯å¤±è´¥: {e}", exc_info=True)
            raise
    
    @classmethod
    def register_client(
        cls, 
        provider: str, 
        client_class: Type[BaseModelClient]
    ) -> None:
        """
        æ³¨å†Œæ–°çš„æ¨¡å‹å®¢æˆ·ç«¯ç±»å‹
        
        Args:
            provider: æä¾›å•†åç§°
            client_class: å®¢æˆ·ç«¯ç±»
        """
        provider = provider.lower()
        cls._client_registry[provider] = client_class
        logger.info(f"å·²æ³¨å†Œæ¨¡å‹å®¢æˆ·ç«¯: {provider}")


# ============================================================================
# ä¾¿æ·å‡½æ•°
# ============================================================================

def get_model_client(task_type: str) -> BaseModelClient:
    """
    è·å–æŒ‡å®šä»»åŠ¡ç±»å‹çš„æ¨¡å‹å®¢æˆ·ç«¯ï¼ˆä¾¿æ·å‡½æ•°ï¼‰
    
    Args:
        task_type: ä»»åŠ¡ç±»å‹ (video_summary, pdf_processing, visual_generation, document_analysis)
        
    Returns:
        æ¨¡å‹å®¢æˆ·ç«¯å®ä¾‹
        
    Example:
        >>> client = get_model_client("visual_generation")
        >>> result = await client.generate_content(prompt)
    """
    config_manager = ModelConfigManager.get_instance()
    config = config_manager.get_config(task_type)
    return ModelClientFactory.create_client(config)


def get_default_client() -> BaseModelClient:
    """
    è·å–é»˜è®¤æ¨¡å‹å®¢æˆ·ç«¯
    
    Returns:
        é»˜è®¤æ¨¡å‹å®¢æˆ·ç«¯å®ä¾‹
    """
    config_manager = ModelConfigManager.get_instance()
    config = config_manager.get_default_config()
    return ModelClientFactory.create_client(config)



